{"cells":[{"source":"# Intermediate Importing Data in Python","metadata":{},"id":"20c73b4c-626a-4674-b2ca-0f4c47f60628","cell_type":"markdown"},{"source":"# Importing the course packages\nimport json\nimport pandas as pd\n\n# Read the Twitter data\ntweets_data = []\ntweets_file = open(\"datasets/tweets.txt\", \"r\")\nfor line in tweets_file:\n    tweet = json.loads(line)\n    tweets_data.append(tweet)\ntweets_file.close()\n\n# Import the other two datasets\nwine = pd.read_csv(\"datasets/winequality-red.csv\", sep=\";\")\nlatitude = pd.read_excel(\"datasets/latitude.xls\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":28,"lastSuccessfullyExecutedCode":"# Importing the course packages\nimport json\nimport pandas as pd\n\n# Read the Twitter data\ntweets_data = []\ntweets_file = open(\"datasets/tweets.txt\", \"r\")\nfor line in tweets_file:\n    tweet = json.loads(line)\n    tweets_data.append(tweet)\ntweets_file.close()\n\n# Import the other two datasets\ndf = pd.read_csv(\"datasets/winequality-red.csv\", sep=\";\")\nxl = pd.read_excel(\"datasets/latitude.xls\")"},"id":"cc8f4fbc-8936-468a-b789-fe76aae1fc03","cell_type":"code","execution_count":8,"outputs":[]},{"source":"## Content \n\n- Importing text files and flat files\n- Importing files in other formats\n- Writing SQL queries\n- Getting data from relational databases\n- Pulling data from the web\n- Pulling data from APIs","metadata":{},"id":"47777ca5-c181-41a3-8eef-b305c567d99d","cell_type":"markdown"},{"source":"# PART ONE","metadata":{},"cell_type":"markdown","id":"059fcc09-218a-4724-80a6-b161b9a39525"},{"source":"## 1 Importing flat files from the web\n\n## You’re already great at importing!\n\n- Flat files such as .txt and .csv \n- Pickled files, Excel spreadsheets, and many others!\n- Data from relational databases\n- You can do all these locally\n- What if your data is online?\n![Screen Shot 2023-08-29 at 11.49.17 AM](Screen%20Shot%202023-08-29%20at%2011.49.17%20AM.png)\n\n## Can you import web data?\n- You can: go to URL and click to download files\n- BUT: not reproducible, not scalable\n\n\n## You’ll learn how to ...\n\n- Import and locally save datasets from the web\n- Load datasets into pandas DataFrames\n- Make HTTP requests (GET requests)\n- Scrape web data such as HTML\n- Parse HTML into useful data (BeautifulSoup)\n- Use the urllib and request spackages\n\n## The `urllib` package\n- Provides interface for fetching data across the web\n- `urlopen()`- accepts URLs instead of file name","metadata":{},"cell_type":"markdown","id":"3a86d1ba-dece-46a7-b561-12d11e488765"},{"source":"## How to automate file download in Python","metadata":{},"cell_type":"markdown","id":"c90c3761-8234-432b-808c-da38564eed4a"},{"source":"from urllib.request import urlretrieve\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\nurlretrieve(url, 'winequality-white.csv')","metadata":{"executionCancelledAt":null,"executionTime":667,"lastExecutedAt":1693307291396,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from urllib.request import urlretrieve\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\nurlretrieve(url, 'winequality-white.csv')"},"cell_type":"code","id":"7894ae5a-a20b-44a3-bb87-00fcebf117f1","execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":"('winequality-white.csv', <http.client.HTTPMessage at 0x7fe3c74883d0>)"},"metadata":{},"execution_count":2}]},{"source":"## Importing flat files from the web: your turn!\nYou are about to import your first file from the web! The flat file you will import will be `'winequality-red.csv'` from the University of California, Irvine's Machine Learning repository. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.\n\nThe URL of the file is\n\n`'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'`  \nAfter you import it, you'll check your working directory to confirm that it is there and then you'll load it into a pandas DataFrame.\n\n### Instructions\n\n- Import the function urlretrieve from the subpackage urllib.request.\n- Assign the URL of the file to the variable url.\n- Use the function urlretrieve() to save the file locally as 'winequality-red.csv'.\n- Execute the remaining code to load 'winequality-red.csv' in a pandas DataFrame and to print its head to the shell.","metadata":{},"cell_type":"markdown","id":"d3ed4a8d-8a2e-4336-97ba-395ec84390cc"},{"source":"# Import package\nfrom urllib.request import urlretrieve\n\n# Import pandas\nimport pandas as pd\n\n# Assign url of file: url\nurl = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n\n# Save file locally\nurlretrieve(url, 'winequality-red.csv')\n\n# Read file into a DataFrame and print its head\ndf = pd.read_csv('winequality-red.csv', sep=';')\nprint(df.head())","metadata":{"executionCancelledAt":null,"executionTime":104,"lastExecutedAt":1693308083858,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import package\nfrom urllib.request import urlretrieve\n\n# Import pandas\nimport pandas as pd\n\n# Assign url of file: url\nurl = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n\n# Save file locally\nurlretrieve(url, 'winequality-red.csv')\n\n# Read file into a DataFrame and print its head\ndf = pd.read_csv('winequality-red.csv', sep=';')\nprint(df.head())","outputsMetadata":{"0":{"height":178,"type":"stream"}}},"cell_type":"code","id":"a44da284-e935-4c0e-9016-ae9c999f544a","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n0            7.4              0.70         0.00  ...       0.56      9.4        5\n1            7.8              0.88         0.00  ...       0.68      9.8        5\n2            7.8              0.76         0.04  ...       0.65      9.8        5\n3           11.2              0.28         0.56  ...       0.58      9.8        6\n4            7.4              0.70         0.00  ...       0.56      9.4        5\n\n[5 rows x 12 columns]\n"}]},{"source":"## Opening and reading flat files from the web\n\nYou have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas. In particular, you can use the function `pd.read_csv()` with the URL as the first argument and the separator sep as the second argument.\n\nThe URL of the file, once again, is  \n`'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'`  \n\n### Instructions\n\n- Assign the URL of the file to the variable `url`.\n- Read file into a DataFrame df using `pd.read_csv()`, recalling that the separator in the file is ';'.\n- Print the head of the DataFrame `df`.\n- Execute the rest of the code to plot histogram of the first feature in the DataFrame `df`.","metadata":{},"cell_type":"markdown","id":"22178f5e-b8ab-4815-b8cd-7527c0f29aa2"},{"source":"# Import packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Assign url of file: url\nurl = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n\n# Read file into a DataFrame: df\ndf = pd.read_csv(url, sep=';')\n\n# Print the head of the DataFrame\nprint(df.head())\n\n# Plot first column of df\ndf.iloc[:, 0].hist()\nplt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\nplt.ylabel('count')\nplt.show()","metadata":{"executionCancelledAt":null,"executionTime":3115,"lastExecutedAt":1693308214186,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Assign url of file: url\nurl = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n\n# Read file into a DataFrame: df\ndf = pd.read_csv(url, sep=';')\n\n# Print the head of the DataFrame\nprint(df.head())\n\n# Plot first column of df\ndf.iloc[:, 0].hist()\nplt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\nplt.ylabel('count')\nplt.show()","outputsMetadata":{"0":{"height":178,"type":"stream"}}},"cell_type":"code","id":"c20d4f76-2788-4e8c-8662-7f205e9e910f","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n0            7.4              0.70         0.00  ...       0.56      9.4        5\n1            7.8              0.88         0.00  ...       0.68      9.8        5\n2            7.8              0.76         0.04  ...       0.65      9.8        5\n3           11.2              0.28         0.56  ...       0.58      9.8        6\n4            7.4              0.70         0.00  ...       0.56      9.4        5\n\n[5 rows x 12 columns]\n"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"source":"## Importing non-flat files from the web\nCongrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the pandas function `pd.read_csv()`. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use `pd.read_excel()` to import an Excel spreadsheet.\n\nThe URL of the spreadsheet is\n\n'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'  \nYour job is to use pd.read_excel() to read in all of its sheets, print the sheet names and then print the head of the first sheet using its name, not its index.\n\nNote that the output of `pd.read_excel()` is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.\n\n### Instructions\n\n- Assign the URL of the file to the variable `url`.\n- Read the file in url into a dictionary xls using `pd.read_excel()` recalling that, in order to import all sheets you need to pass None to the argument sheet_name.\n- Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary xls.\n- Print the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is '1700'","metadata":{},"cell_type":"markdown","id":"fb9362e9-6d7f-4dc3-84aa-05e998047557"},{"source":"# Import package\nimport pandas as pd\n\n# Assign url of file: url\nurl = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n\n# Read in all sheets of Excel file: xls\nxls = pd.read_excel(url, sheet_name=None)\n\n# Print the sheetnames to the shell\nprint(xls.keys())\n\n# Print the head of the first sheet (using its name, NOT its index)\nprint(xls['1700'].head())","metadata":{"executionCancelledAt":null,"executionTime":97,"lastExecutedAt":1693308852796,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import package\nimport pandas as pd\n\n# Assign url of file: url\nurl = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n\n# Read in all sheets of Excel file: xls\nxls = pd.read_excel(url, sheet_name=None)\n\n# Print the sheetnames to the shell\nprint(xls.keys())\n\n# Print the head of the first sheet (using its name, NOT its index)\nprint(xls['1700'].head())","outputsMetadata":{"0":{"height":158,"type":"stream"}}},"cell_type":"code","id":"eceba452-019c-41f2-848d-eaa98d2b0bef","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"dict_keys(['1700', '1900'])\n                 country       1700\n0            Afghanistan  34.565000\n1  Akrotiri and Dhekelia  34.616667\n2                Albania  41.312000\n3                Algeria  36.720000\n4         American Samoa -14.307000\n"}]},{"source":"## 2 HTTP requests to import files from the web\n\n## URL\n\n- Uniform/Universal Resource Locator\n- References to web resources\n- Focus: web addresses\n- Ingredients:\n    - Protocol identifier-http:\n    - Resource name- datacamp.com \n- These specify web addresses uniquely\n\n## HTTP\n\n- HyperText Transfer Protocol\n- Foundation of data communication for the web\n- HTTPS- more secure form of HTTP\n- Going to a website = sending HTTP request\n    - GET request\n- `urlretrieve()` performs a GET request\n- HTML-HyperText Markup Language\n","metadata":{},"cell_type":"markdown","id":"e3171073-db87-4837-9ec6-dedd923bf99e"},{"source":"## GET requests using `urllib`","metadata":{},"cell_type":"markdown","id":"5162e6f2-b715-43f0-b6c4-e6f8f3705c77"},{"source":"from urllib.request import urlopen, Request\nurl = \"https://www.wikipedia.org/\"\nrequest = Request(url)\nresponse = urlopen(request)\nhtml = response.read()\nresponse.close()","metadata":{"executionCancelledAt":null,"executionTime":139,"lastExecutedAt":1693307316727,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from urllib.request import urlopen, Request\nurl = \"https://www.wikipedia.org/\"\nrequest = Request(url)\nresponse = urlopen(request)\nhtml = response.read()\nresponse.close()"},"cell_type":"code","id":"64258384-197e-44ff-840d-4f07b261ae50","execution_count":3,"outputs":[]},{"source":"## GET requests using requests\n\n- One of the most downloaded Python packages","metadata":{},"cell_type":"markdown","id":"6649c643-b627-4e10-8a0a-88bbff47147a"},{"source":"import requests\nurl = \"https://www.wikipedia.org/\"\nr = requests.get(url)\ntext = r.text","metadata":{"executionCancelledAt":null,"executionTime":99,"lastExecutedAt":1693307323274,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import requests\nurl = \"https://www.wikipedia.org/\"\nr = requests.get(url)\ntext = r.text"},"cell_type":"code","id":"f4703b95-346b-4bd5-b804-c0aa3aab273a","execution_count":4,"outputs":[]},{"source":"## Performing HTTP requests in Python using urllib\n\nNow that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, `\"https://campus.datacamp.com/courses/1606/4135?ex=2\"`.  \n\nIn the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.\n\n### Instructions\n\n- Import the functions `urlopen` and Request from the subpackage `urllib.request`.\n- Package the request to the url `\"https://campus.datacamp.com/courses/1606/4135?ex=2\"` using the function Request() and assign it to request.\n- Send the request and catch the response in the variable response with the function `urlopen()`.\n- Run the rest of the code to see the datatype of response and to close the connection!","metadata":{},"cell_type":"markdown","id":"c560c636-59c8-41d6-b6cc-8823ab1c634f"},{"source":"# Import packages\nfrom urllib.request import urlopen, Request\n\n# Specify the url\nurl = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n\n# This packages the request: request\nrequest = Request(url)\n\n# Sends the request and catches the response: response\nresponse = urlopen(request)\n\n# Print the datatype of response\nprint(type(response))\n\n# Be polite and close the response!\nresponse.close()","metadata":{},"cell_type":"code","id":"892b280d-1259-4f1d-b862-b0381d29d3f4","execution_count":null,"outputs":[]},{"source":"## Printing HTTP request results in Python using `urllib`\n\nYou have just packaged and sent a GET request to \"https://campus.datacamp.com/courses/1606/4135?ex=2\" and then caught the response. You saw that such a response is a http.client.HTTPResponse object. The question remains: what can you do with this response?\n\nWell, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a http.client.HTTPResponse object has an associated `read()` method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.\n\n### Instructions\n\n- Send the request and catch the response in the variable response with the function `urlopen()`, as in the previous exercise.\n- Extract the response using the `read()` method and store the result in the variable html.\n- Print the string `html`.\n- Hit submit to perform all of the above and to close the response: be tidy!","metadata":{},"cell_type":"markdown","id":"bcccac7b-0b95-44e5-ab4a-9dcd7c5ebf17"},{"source":"# Import packages\nfrom urllib.request import urlopen, Request\n\n# Specify the url\nurl = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n\n# This packages the request\nrequest = Request(url)\n\n# Sends the request and catches the response: response\nresponse = urlopen(request)\n\n# Extract the response: html\nhtml = response.read()\n\n# Print the html\nprint(html)\n\n# Be polite and close the response!\nresponse.close()","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1693309650149,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"bc936bf3-d2bd-433f-8f59-d3f901e053eb","execution_count":9,"outputs":[]},{"source":"## Performing HTTP requests in Python using requests\n\nNow that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their \"http://www.datacamp.com/teach/documentation\" page.\n\nNote that unlike in the previous exercises using `urllib`, you don't have to close the connection when using requests!\n\n### Instructions\n\n- Import the package `requests`.\n- Assign the URL of interest to the variable `url`.\n- Package the `request` to the URL, send the `request` and catch the `response` with a single function `requests.get()`, assigning the response to the variable `r`.\n- Use the text attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable text.\n- Hit submit to print the HTML of the webpage.","metadata":{},"cell_type":"markdown","id":"2851403f-bdf5-4080-96d5-04918d7383d0"},{"source":"# Import package\nimport requests\n\n# Specify the url: url\nurl = \"http://www.datacamp.com/teach/documentation\"\n\n\n# Packages the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Extract the response: text\ntext = r.text\n\n# Print the html\nprint(text)","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1693309945255,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"f14f0612-2528-461c-b6cd-1674a303f186","execution_count":10,"outputs":[]},{"source":"## 3 Scraping the web in Python\n\n## HTML\n\n- Mix of unstructured and structured data\n- Structured data:\n    - Has pre-defined data model,or\n    - Organized in a defined manner\n- Unstructured data: neither of these properties\n\n![Screen Shot 2023-08-29 at 12.04.35 PM](Screen%20Shot%202023-08-29%20at%2012.04.35%20PM.png)\n\n\n## BeautifulSoup\n\n- Parse and extract structured data from HTML\n![Screen Shot 2023-08-29 at 12.05.48 PM](Screen%20Shot%202023-08-29%20at%2012.05.48%20PM.png)\n- Make tag soup beautiful and extract information","metadata":{},"cell_type":"markdown","id":"e0bd3c87-953e-44a8-834f-37843da1417c"},{"source":"## BeautifulSoup","metadata":{},"cell_type":"markdown","id":"256d502a-8abd-4aa6-b602-4a2ec5f362ee"},{"source":"from bs4 import BeautifulSoup\nimport requests\nurl = 'https://www.crummy.com/software/BeautifulSoup/'\nr = requests.get(url)\nhtml_doc = r.text\nsoup = BeautifulSoup(html_doc)\n## Prettified Soup\nprint(soup.prettify())","metadata":{"executionCancelledAt":null,"executionTime":520,"lastExecutedAt":1693307276639,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from bs4 import BeautifulSoup\nimport requests\nurl = 'https://www.crummy.com/software/BeautifulSoup/'\nr = requests.get(url)\nhtml_doc = r.text\nsoup = BeautifulSoup(html_doc)\n## Prettified Soup\nprint(soup.prettify())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"bd0f972c-1ac2-4559-8aae-a083b4c05dfc","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n<html>\n <head>\n  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n  <title>\n   Beautiful Soup: We called him Tortoise because he taught us.\n  </title>\n  <link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n  <link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n  <meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n  <meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n  <meta content=\"Leonard Richardson\" name=\"author\"/>\n </head>\n <body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n  <style>\n   #tidelift { }\n\n#tidelift a {\n border: 1px solid #666666;\n margin-left: auto;\n padding: 10px;\n text-decoration: none;\n}\n\n#tidelift .cta {\n background: url(\"tidelift.svg\") no-repeat;\n padding-left: 30px;\n}\n  </style>\n  <img align=\"right\" src=\"10.1.jpg\" width=\"250\"/>\n  <br/>\n  <p>\n   [\n   <a href=\"#Download\">\n    Download\n   </a>\n   |\n   <a href=\"bs4/doc/\">\n    Documentation\n   </a>\n   |\n   <a href=\"#HallOfFame\">\n    Hall of Fame\n   </a>\n   |\n   <a href=\"enterprise.html\">\n    For enterprise\n   </a>\n   |\n   <a href=\"https://code.launchpad.net/beautifulsoup\">\n    Source\n   </a>\n   |\n   <a href=\"https://git.launchpad.net/beautifulsoup/tree/CHANGELOG\">\n    Changelog\n   </a>\n   |\n   <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n    Discussion group\n   </a>\n   |\n   <a href=\"zine/\">\n    Zine\n   </a>\n   ]\n  </p>\n  <div align=\"center\">\n   <a href=\"bs4/download/\">\n    <h1>\n     Beautiful Soup\n    </h1>\n   </a>\n  </div>\n  <p>\n   You didn't write that awful page. You're just trying to get some\ndata out of it. Beautiful Soup is here to help. Since 2004, it's been\nsaving programmers hours or days of work on quick-turnaround\nscreen scraping projects.\n  </p>\n  <p>\n   Beautiful Soup is a Python library designed for quick turnaround\nprojects like screen-scraping. Three features make it powerful:\n  </p>\n  <ol>\n   <li>\n    Beautiful Soup provides a few simple methods and Pythonic idioms\nfor navigating, searching, and modifying a parse tree: a toolkit for\ndissecting a document and extracting what you need. It doesn't take\nmuch code to write an application\n   </li>\n   <li>\n    Beautiful Soup automatically converts incoming documents to\nUnicode and outgoing documents to UTF-8. You don't have to think\nabout encodings, unless the document doesn't specify an encoding and\nBeautiful Soup can't detect one. Then you just have to specify the\noriginal encoding.\n   </li>\n   <li>\n    Beautiful Soup sits on top of popular Python parsers like\n    <a href=\"http://lxml.de/\">\n     lxml\n    </a>\n    and\n    <a href=\"http://code.google.com/p/html5lib/\">\n     html5lib\n    </a>\n    , allowing you\nto try out different parsing strategies or trade speed for\nflexibility.\n   </li>\n  </ol>\n  <p>\n   Beautiful Soup parses anything you give it, and does the tree\ntraversal stuff for you. You can tell it \"Find all the links\", or\n\"Find all the links of class\n   <tt>\n    externalLink\n   </tt>\n   \", or \"Find all the\nlinks whose urls match \"foo.com\", or \"Find the table heading that's\ngot bold text, then give me that text.\"\n  </p>\n  <p>\n   Valuable data that was once locked up in poorly-designed websites\nis now within your reach. Projects that would have taken hours take\nonly minutes with Beautiful Soup.\n  </p>\n  <p>\n   Interested?\n   <a href=\"bs4/doc/\">\n    Read more.\n   </a>\n  </p>\n  <h3>\n   Getting and giving support\n  </h3>\n  <div align=\"center\" id=\"tidelift\">\n   <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=enterprise\" target=\"_blank\">\n    <span class=\"cta\">\n     Beautiful Soup for enterprise available via Tidelift\n    </span>\n   </a>\n  </div>\n  <p>\n   If you have questions, send them to\n   <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n    the discussion\ngroup\n   </a>\n   . If you find a bug,\n   <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n    file it on Launchpad\n   </a>\n   . If it's a security vulnerability, report it confidentially through\n   <a href=\"https://tidelift.com/security\">\n    Tidelift\n   </a>\n   .\n  </p>\n  <p>\n   If you use Beautiful Soup as part of your work, please consider a\n   <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">\n    Tidelift subscription\n   </a>\n   . This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n  </p>\n  <p>\n   If Beautiful Soup is useful to you on a personal level, you might like to read\n   <a href=\"zine/\">\n    <i>\n     Tool Safety\n    </i>\n   </a>\n   , a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n  </p>\n  <a name=\"Download\">\n   <h2>\n    Download Beautiful Soup\n   </h2>\n  </a>\n  <p>\n   The current release is\n   <a href=\"bs4/download/\">\n    Beautiful Soup\n4.12.2\n   </a>\n   (April 7, 2023). You can install Beautiful Soup 4 with\n   <code>\n    pip install beautifulsoup4\n   </code>\n   .\n  </p>\n  <p>\n   In Debian and Ubuntu, Beautiful Soup is available as the\n   <code>\n    python3-bs4\n   </code>\n   package. In Fedora it's\navailable as the\n   <code>\n    python3-beautifulsoup4\n   </code>\n   package.\n  </p>\n  <p>\n   Beautiful Soup is licensed under the MIT license, so you can also\ndownload the tarball, drop the\n   <code>\n    bs4/\n   </code>\n   directory into almost\nany Python application (or into your library path) and start using it\nimmediately.\n  </p>\n  <p>\n   Beautiful Soup 4 is supported on Python versions 3.6 and\ngreater. Support for Python 2 was discontinued on January 1, 2021—one\nyear after the Python 2 sunsetting date.\n  </p>\n  <h3>\n   Beautiful Soup 3\n  </h3>\n  <p>\n   Beautiful Soup 3 was the official release line of Beautiful Soup\nfrom May 2006 to March 2012. It does not support Python 3 and was\ndiscontinued or January 1, 2021—one year after the Python 2\nsunsetting date. If you have any active projects using Beautiful Soup\n3, you should migrate to Beautiful Soup 4 as part of your Python 3\nconversion.\n  </p>\n  <p>\n   <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">\n    Here's\nthe Beautiful Soup 3 documentation.\n   </a>\n  </p>\n  <p>\n   The current and hopefully final release of Beautiful Soup 3 is\n   <a href=\"download/3.x/BeautifulSoup-3.2.2.tar.gz\">\n    3.2.2\n   </a>\n   (October 5,\n2019). It's the\n   <code>\n    BeautifulSoup\n   </code>\n   package on pip. It's also\navailable as\n   <code>\n    python-beautifulsoup\n   </code>\n   in Debian and Ubuntu,\nand as\n   <code>\n    python-BeautifulSoup\n   </code>\n   in Fedora.\n  </p>\n  <p>\n   Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n  </p>\n  <p>\n   Beautiful Soup 3, like Beautiful Soup 4, is\n   <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&amp;utm_medium=referral&amp;utm_campaign=website\">\n    supported through Tidelift\n   </a>\n   .\n  </p>\n  <a name=\"HallOfFame\">\n   <h2>\n    Hall of Fame\n   </h2>\n  </a>\n  <p>\n   Over the years, Beautiful Soup has been used in hundreds of\ndifferent projects. There's no way I can list them all, but I want to\nhighlight a few high-profile projects. Beautiful Soup isn't what makes\nthese projects interesting, but it did make their completion easier:\n  </p>\n  <ul>\n   <li>\n    <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\n     \"Movable\n Type\"\n    </a>\n    , a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n   </li>\n   <li>\n    Jiabao Lin's\n    <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">\n     DXY-COVID-19-Crawler\n    </a>\n    uses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source:\n    <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\n     \"How open source software is fighting COVID-19\"\n    </a>\n    )\n   </li>\n   <li>\n    Reddit uses Beautiful Soup to\n    <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">\n     parse\na page that's been linked to and find a representative image\n    </a>\n    .\n   </li>\n   <li>\n    Alexander Harrowell uses Beautiful Soup to\n    <a href=\"http://www.harrowell.org.uk/viktormap.html\">\n     track the business\n activities\n    </a>\n    of an arms merchant.\n   </li>\n   <li>\n    The developers of Python itself used Beautiful Soup to\n    <a href=\"http://svn.python.org/view/tracker/importer/\">\n     migrate the Python\nbug tracker from Sourceforge to Roundup\n    </a>\n    .\n   </li>\n   <li>\n    The\n    <a href=\"http://www2.ljworld.com/\">\n     Lawrence Journal-World\n    </a>\n    uses Beautiful Soup to\n    <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">\n     gather\nstatewide election results\n    </a>\n    .\n   </li>\n   <li>\n    The\n    <a href=\"http://esrl.noaa.gov/gsd/fab/\">\n     NOAA's Forecast\nApplications Branch\n    </a>\n    uses Beautiful Soup in\n    <a href=\"http://laps.noaa.gov/topograbber/\">\n     TopoGrabber\n    </a>\n    , a script for\ndownloading \"high resolution USGS datasets.\"\n   </li>\n  </ul>\n  <p>\n   If you've used Beautiful Soup in a project you'd like me to know\nabout, please do send email to me or\n   <a href=\"http://groups.google.com/group/beautifulsoup/\">\n    the discussion\ngroup\n   </a>\n   .\n  </p>\n  <h2>\n   Development\n  </h2>\n  <p>\n   Development happens at\n   <a href=\"https://launchpad.net/beautifulsoup\">\n    Launchpad\n   </a>\n   . You can\n   <a href=\"https://code.launchpad.net/beautifulsoup/\">\n    get the source\ncode\n   </a>\n   or\n   <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n    file\nbugs\n   </a>\n   .\n  </p>\n  <hr/>\n  <table>\n   <tr>\n    <td valign=\"top\">\n     <p>\n      This document is part of Crummy, the webspace of\n      <a href=\"/self/\">\n       Leonard Richardson\n      </a>\n      (\n      <a href=\"/self/contact.html\">\n       contact information\n      </a>\n      ). It was last modified on Friday, April 07 2023, 19:56:57 Nowhere Standard Time and last built on Tuesday, August 29 2023, 11:00:01 Nowhere Standard Time.\n     </p>\n     <p>\n     </p>\n     <table class=\"licenseText\">\n      <tr>\n       <td>\n        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n         <img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/>\n        </a>\n       </td>\n       <td valign=\"top\">\n        Crummy is © 1996-2023 Leonard Richardson. Unless otherwise noted, all text licensed under a\n        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n         Creative Commons License\n        </a>\n        .\n       </td>\n      </tr>\n     </table>\n     <!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>-->\n    </td>\n    <td valign=\"top\">\n     <p>\n      <b>\n       Document tree:\n      </b>\n     </p>\n     <dl>\n      <dd>\n       <a href=\"http://www.crummy.com/\">\n        http://www.crummy.com/\n       </a>\n       <dl>\n        <dd>\n         <a href=\"http://www.crummy.com/software/\">\n          software/\n         </a>\n         <dl>\n          <dd>\n           <a href=\"http://www.crummy.com/software/BeautifulSoup/\">\n            BeautifulSoup/\n           </a>\n          </dd>\n         </dl>\n        </dd>\n       </dl>\n      </dd>\n     </dl>\n     Site Search:\n     <form action=\"/search/\" method=\"get\">\n      <input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n     </form>\n    </td>\n   </tr>\n  </table>\n </body>\n</html>\n\n"}]},{"source":"## Exploring BeautifulSoup\n\n- Many methods such as:\n- `find_all()`","metadata":{},"cell_type":"markdown","id":"1bd56875-90ea-4d1f-8c7b-862f4cc9ddf0"},{"source":"print(soup.title)\nprint(soup.get_text())\nfor link in soup.find_all('a'):    \n    print(link.get('href'))","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1693307452602,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(soup.title)\nprint(soup.get_text())\nfor link in soup.find_all('a'):    \n    print(link.get('href'))","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"a04e7803-2456-482c-9dac-d4ab8824863d","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n\n\n\nBeautiful Soup: We called him Tortoise because he taught us.\n\n\n\n\n\n\n\n\n\n[ Download | Documentation | Hall of Fame | For enterprise | Source | Changelog | Discussion group  | Zine ]\n\nBeautiful Soup\n\nYou didn't write that awful page. You're just trying to get some\ndata out of it. Beautiful Soup is here to help. Since 2004, it's been\nsaving programmers hours or days of work on quick-turnaround\nscreen scraping projects.\nBeautiful Soup is a Python library designed for quick turnaround\nprojects like screen-scraping. Three features make it powerful:\n\n\nBeautiful Soup provides a few simple methods and Pythonic idioms\nfor navigating, searching, and modifying a parse tree: a toolkit for\ndissecting a document and extracting what you need. It doesn't take\nmuch code to write an application\n\nBeautiful Soup automatically converts incoming documents to\nUnicode and outgoing documents to UTF-8. You don't have to think\nabout encodings, unless the document doesn't specify an encoding and\nBeautiful Soup can't detect one. Then you just have to specify the\noriginal encoding.\n\nBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you\nto try out different parsing strategies or trade speed for\nflexibility.\n\n\nBeautiful Soup parses anything you give it, and does the tree\ntraversal stuff for you. You can tell it \"Find all the links\", or\n\"Find all the links of class externalLink\", or \"Find all the\nlinks whose urls match \"foo.com\", or \"Find the table heading that's\ngot bold text, then give me that text.\"\n\nValuable data that was once locked up in poorly-designed websites\nis now within your reach. Projects that would have taken hours take\nonly minutes with Beautiful Soup.\n\nInterested? Read more.\nGetting and giving support\n\n\n\n  Beautiful Soup for enterprise available via Tidelift\n \n\n\nIf you have questions, send them to the discussion\ngroup. If you find a bug, file it on Launchpad. If it's a security vulnerability, report it confidentially through Tidelift.\nIf you use Beautiful Soup as part of your work, please consider a Tidelift subscription. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n\n\nIf Beautiful Soup is useful to you on a personal level, you might like to read Tool Safety, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\nDownload Beautiful Soup\nThe current release is Beautiful Soup\n4.12.2 (April 7, 2023). You can install Beautiful Soup 4 with\npip install beautifulsoup4.\n\nIn Debian and Ubuntu, Beautiful Soup is available as the\npython3-bs4 package. In Fedora it's\navailable as the python3-beautifulsoup4 package.\n\nBeautiful Soup is licensed under the MIT license, so you can also\ndownload the tarball, drop the bs4/ directory into almost\nany Python application (or into your library path) and start using it\nimmediately.\n\nBeautiful Soup 4 is supported on Python versions 3.6 and\ngreater. Support for Python 2 was discontinued on January 1, 2021—one\nyear after the Python 2 sunsetting date.\n\nBeautiful Soup 3\nBeautiful Soup 3 was the official release line of Beautiful Soup\nfrom May 2006 to March 2012. It does not support Python 3 and was\ndiscontinued or January 1, 2021—one year after the Python 2\nsunsetting date. If you have any active projects using Beautiful Soup\n3, you should migrate to Beautiful Soup 4 as part of your Python 3\nconversion.\n\nHere's\nthe Beautiful Soup 3 documentation.\nThe current and hopefully final release of Beautiful Soup 3 is 3.2.2 (October 5,\n2019). It's the BeautifulSoup package on pip. It's also\navailable as python-beautifulsoup in Debian and Ubuntu,\nand as python-BeautifulSoup in Fedora.\n\nOnce Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n\nBeautiful Soup 3, like Beautiful Soup 4, is supported through Tidelift.\nHall of Fame\nOver the years, Beautiful Soup has been used in hundreds of\ndifferent projects. There's no way I can list them all, but I want to\nhighlight a few high-profile projects. Beautiful Soup isn't what makes\nthese projects interesting, but it did make their completion easier:\n\n\n\"Movable\n Type\", a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n\nJiabao Lin's DXY-COVID-19-Crawler\nuses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source: \"How open source software is fighting COVID-19\")\n\nReddit uses Beautiful Soup to parse\na page that's been linked to and find a representative image.\n\nAlexander Harrowell uses Beautiful Soup to track the business\n activities of an arms merchant.\n\nThe developers of Python itself used Beautiful Soup to migrate the Python\nbug tracker from Sourceforge to Roundup.\n\nThe Lawrence Journal-World\nuses Beautiful Soup to gather\nstatewide election results.\n\nThe NOAA's Forecast\nApplications Branch uses Beautiful Soup in TopoGrabber, a script for\ndownloading \"high resolution USGS datasets.\"\n\n\nIf you've used Beautiful Soup in a project you'd like me to know\nabout, please do send email to me or the discussion\ngroup.\n\nDevelopment\nDevelopment happens at Launchpad. You can get the source\ncode or file\nbugs.\nThis document is part of Crummy, the webspace of Leonard Richardson (contact information). It was last modified on Friday, April 07 2023, 19:56:57 Nowhere Standard Time and last built on Tuesday, August 29 2023, 11:00:01 Nowhere Standard Time.Crummy is © 1996-2023 Leonard Richardson. Unless otherwise noted, all text licensed under a Creative Commons License.Document tree:\nhttp://www.crummy.com/software/BeautifulSoup/\n\n\n\n\nSite Search:\n\n\n\n\n\n\n\n\n\n\n#Download\nbs4/doc/\n#HallOfFame\nenterprise.html\nhttps://code.launchpad.net/beautifulsoup\nhttps://git.launchpad.net/beautifulsoup/tree/CHANGELOG\nhttps://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\nzine/\nbs4/download/\nhttp://lxml.de/\nhttp://code.google.com/p/html5lib/\nbs4/doc/\nhttps://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise\nhttps://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\nhttps://bugs.launchpad.net/beautifulsoup/\nhttps://tidelift.com/security\nhttps://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\nzine/\nNone\nbs4/download/\nhttp://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\ndownload/3.x/BeautifulSoup-3.2.2.tar.gz\nhttps://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website\nNone\nhttp://www.nytimes.com/2007/10/25/arts/design/25vide.html\nhttps://github.com/BlankerL/DXY-COVID-19-Crawler\nhttps://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\nhttps://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\nhttp://www.harrowell.org.uk/viktormap.html\nhttp://svn.python.org/view/tracker/importer/\nhttp://www2.ljworld.com/\nhttp://www.b-list.org/weblog/2010/nov/02/news-done-broke/\nhttp://esrl.noaa.gov/gsd/fab/\nhttp://laps.noaa.gov/topograbber/\nhttp://groups.google.com/group/beautifulsoup/\nhttps://launchpad.net/beautifulsoup\nhttps://code.launchpad.net/beautifulsoup/\nhttps://bugs.launchpad.net/beautifulsoup/\n/self/\n/self/contact.html\nhttp://creativecommons.org/licenses/by-sa/2.0/\nhttp://creativecommons.org/licenses/by-sa/2.0/\nhttp://www.crummy.com/\nhttp://www.crummy.com/software/\nhttp://www.crummy.com/software/BeautifulSoup/\n"}]},{"source":"## Parsing HTML with `BeautifulSoup`\n\nIn this interactive exercise, you'll learn how to use the `BeautifulSoup` package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own Benevolent Dictator for Life. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.\n\nThe URL of interest is url = `'https://www.python.org/~guido/'`.\n\n### Instructions\n\n- Import the function `BeautifulSoup` from the package `bs4`.\n- Assign the URL of interest to the variable `url`.\n- Package the request to the URL, send the request and catch the response with a single function `requests.get()`, assigning the response to the variable `r`.\n- Use the text attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable `html_doc`.\n- Create a `BeautifulSoup` object soup from the resulting HTML using the function `BeautifulSoup()`.\n- Use the method `prettify()` on soup and assign the result to `pretty_soup`.\n- Hit submit to print to prettified HTML to your shell!","metadata":{},"cell_type":"markdown","id":"47693e98-8a20-4f4f-bf29-73c57f335deb"},{"source":"# Import packages\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Specify url: url\nurl = 'https://www.python.org/~guido/'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Extracts the response as html: html_doc\nhtml_doc = r.text\n\n# Create a BeautifulSoup object from the HTML: soup\nsoup = BeautifulSoup(html_doc)\n\n# Prettify the BeautifulSoup object: pretty_soup\npretty_soup = soup.prettify()\n\n# Print the response\nprint(pretty_soup)","metadata":{},"cell_type":"code","id":"f2c61b88-15d4-4176-9d25-3ab1270152db","execution_count":null,"outputs":[]},{"source":"## Turning a webpage into data using BeautifulSoup: getting the text\nAs promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title.\n\n### Instructions\n\n- In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function `BeautifulSoup()` and to assign the resulting soup to the variable `soup`.\n- Extract the title from the HTML soup soup using the attribute title and assign the result to `guido_title`.\n- Print the title of Guido's webpage to the shell using the `print()` function.\n- Extract the text from the HTML soup soup using the method `get_text()` and assign to `guido_text`.\n- Hit submit to print the text from Guido's webpage to the shell.","metadata":{},"cell_type":"markdown","id":"1dd43e13-f7e1-4d52-aea2-2693c3d77a66"},{"source":"# Import packages\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Specify url: url\nurl = 'https://www.python.org/~guido/'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Extract the response as html: html_doc\nhtml_doc = r.text\n\n# Create a BeautifulSoup object from the HTML: soup\nsoup = BeautifulSoup(html_doc)\n\n# Get the title of Guido's webpage: guido_title\nguido_title = soup.title\n\n# Print the title of Guido's webpage to the shell\nprint(guido_title)\n\n# Get Guido's text: guido_text\nguido_text = soup.get_text()\n\n# Print Guido's text to the shell\nprint(guido_text)","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1693311141253,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import packages\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Specify url: url\nurl = 'https://www.python.org/~guido/'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Extract the response as html: html_doc\nhtml_doc = r.text\n\n# Create a BeautifulSoup object from the HTML: soup\nsoup = BeautifulSoup(html_doc)\n\n# Get the title of Guido's webpage: guido_title\nguido_title = soup.title\n\n# Print the title of Guido's webpage to the shell\nprint(guido_title)\n\n# Get Guido's text: guido_text\nguido_text = soup.get_text()\n\n# Print Guido's text to the shell\nprint(guido_text)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"9ce1c39f-4081-4fd1-8167-ab6fd6c03cce","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"<title>Guido's Personal Home Page</title>\n\n\nGuido's Personal Home Page\n\n\n\n\n\nGuido van Rossum - Personal Home Page\n\n\n\"Gawky and proud of it.\"\nWho I Am\nRead\nmy \"King's\nDay Speech\" for some inspiration.\n\nI am the author of the Python\nprogramming language.  See also my resume\nand my publications list, a brief bio, assorted writings, presentations and interviews (all about Python), some\npictures of me,\nmy new blog, and\nmy old\nblog on Artima.com.  I am\n@gvanrossum on Twitter.\n\nI am currently a Distinguished Engineer at Microsoft.\nI have worked for Dropbox, Google, Elemental Security, Zope\nCorporation, BeOpen.com, CNRI, CWI, and SARA.  (See\nmy resume.)  I created Python while at CWI.\n\nHow to Reach Me\nYou can send email for me to guido (at) python.org.\nI read everything sent there, but I receive too much email to respond\nto everything.\n\nMy Name\nMy name often poses difficulties for Americans.\n\nPronunciation: in Dutch, the \"G\" in Guido is a hard G,\npronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\nsound clip.)  However, if you're\nAmerican, you may also pronounce it as the Italian \"Guido\".  I'm not\ntoo worried about the associations with mob assassins that some people\nhave. :-)\n\nSpelling: my last name is two words, and I'd like to keep it\nthat way, the spelling on some of my credit cards notwithstanding.\nDutch spelling rules dictate that when used in combination with my\nfirst name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\nlast name is used alone to refer to me, it is capitalized, for\nexample: \"As usual, Van Rossum was right.\"\n\nAlphabetization: in America, I show up in the alphabet under\n\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\nme under \"G\" in their address book...\n\n\nMore Hyperlinks\n\nHere's a collection of essays relating to Python\nthat I've written, including the foreword I wrote for Mark Lutz' book\n\"Programming Python\".\nI own the official \nPython license.\n\nThe Audio File Formats FAQ\nI was the original creator and maintainer of the Audio File Formats\nFAQ.  It is now maintained by Chris Bagwell\nat http://www.cnpbagwell.com/audio-faq.  And here is a link to\nSOX, to which I contributed\nsome early code.\n\n\n\n\n\"On the Internet, nobody knows you're\na dog.\"\n\n\n\n"}]},{"source":"## Turning a webpage into data using `BeautifulSoup`: getting the hyperlinks\n\nIn this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method `find_all()`.\n\n### Instructions\n\n- Use the method `find_all()` to find all hyperlinks in soup, remembering that hyperlinks are defined by the HTML tag `<a>` but passed to `find_all()` without angle brackets; store the result in the variable `a_tags`.\n- The variable `a_tags` is a results set: your job now is to enumerate over it, using a `for` loop and to print the actual URLs of the hyperlinks; to do this, for every element link in `a_tags`, you want to `print()` `link.get('href')`.","metadata":{},"cell_type":"markdown","id":"5a542182-44b1-48a1-84a1-7da09b13f757"},{"source":"# Import packages\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Specify url\nurl = 'https://www.python.org/~guido/'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Extracts the response as html: html_doc\nhtml_doc = r.text\n\n# create a BeautifulSoup object from the HTML: soup\nsoup = BeautifulSoup(html_doc)\n\n# Print the title of Guido's webpage\nprint(soup.title)\n\n# Find all 'a' tags (which define hyperlinks): a_tags\na_tags = soup.find_all('a')\n\n# Print the URLs to the shell\nfor link in a_tags:\n    print(link.get('href'))","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1693311496264,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import packages\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Specify url\nurl = 'https://www.python.org/~guido/'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Extracts the response as html: html_doc\nhtml_doc = r.text\n\n# create a BeautifulSoup object from the HTML: soup\nsoup = BeautifulSoup(html_doc)\n\n# Print the title of Guido's webpage\nprint(soup.title)\n\n# Find all 'a' tags (which define hyperlinks): a_tags\na_tags = soup.find_all('a')\n\n# Print the URLs to the shell\nfor link in a_tags:\n    print(link.get('href'))","outputsMetadata":{"0":{"height":500,"type":"stream"}}},"cell_type":"code","id":"abe1f4e1-bf49-4f18-9ca2-a8fb2da88d16","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"<title>Guido's Personal Home Page</title>\npics.html\npics.html\nhttp://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\nimages/df20000406.jpg\nhttp://neopythonic.blogspot.com/2016/04/kings-day-speech.html\nhttp://www.python.org\nResume.html\nPublications.html\nbio.html\nhttp://legacy.python.org/doc/essays/\nhttp://legacy.python.org/doc/essays/ppt/\ninterviews.html\npics.html\nhttp://neopythonic.blogspot.com\nhttp://www.artima.com/weblogs/index.jsp?blogger=12088\nhttps://twitter.com/gvanrossum\nResume.html\nguido.au\nhttp://legacy.python.org/doc/essays/\nimages/license.jpg\nhttp://www.cnpbagwell.com/audio-faq\nhttp://sox.sourceforge.net/\nimages/internetdog.gif\n"}]},{"source":"# PART TWO","metadata":{},"cell_type":"markdown","id":"46ea0d45-18ea-47d8-863e-15f9e8fe8b49"},{"source":"## 1 Introduction to APIs and JSONs\n\n## APIs\n- Application Programming Interface \n- Protocols and routines \n    - Building and interacting with software applications\n\n![Screen Shot 2023-08-29 at 1.21.32 PM](Screen%20Shot%202023-08-29%20at%201.21.32%20PM.png)\n\n![Screen Shot 2023-08-29 at 1.21.41 PM](Screen%20Shot%202023-08-29%20at%201.21.41%20PM.png)\n\n\n## JSONs\n- JavaScript Object Notation\n- Real-time server-to-browser communication\n- Douglas Crockford\n- Human readable\n\n![Screen Shot 2023-08-29 at 1.21.57 PM](Screen%20Shot%202023-08-29%20at%201.21.57%20PM.png)\n","metadata":{},"cell_type":"markdown","id":"95bb6fdc-5837-48be-afeb-6638ac2fe78a"},{"source":"## Loading JSONs in Python","metadata":{},"cell_type":"markdown","id":"1792a222-3b84-4db7-8cee-13049d018de9"},{"source":"import json\nwith open('snakes.json', 'r') as json_file:        \n    json_data = json.load(json_file)\n    \ntype(json_data)","metadata":{},"cell_type":"code","id":"7c945891-408e-458c-bf8f-ab5c8d083ba1","execution_count":null,"outputs":[]},{"source":"## Exploring JSONs in Python","metadata":{},"cell_type":"markdown","id":"4c1d20bd-7daa-47c7-9b34-d75741a4ac17"},{"source":"for key, value in json_data.items():    \n    print(key + ':', value)","metadata":{},"cell_type":"code","id":"036a0983-2ba0-4f55-bff0-80fac1837d13","execution_count":null,"outputs":[]},{"source":"## Loading and exploring a JSON\nNow that you know what a JSON is, you'll load one into your Python environment and explore it yourself. Here, you'll load the JSON 'a_movie.json' into the variable json_data, which will be a dictionary. You'll then explore the JSON contents by printing the key-value pairs of json_data to the shell.\n\n### Instructions\n\n- Load the JSON `'a_movie.json'` into the variable json_data within the context provided by the with statement. To do so, use the function `json.load()` within the context manager.\n- Use a for loop to print all key-value pairs in the dictionary json_data. Recall that you can access a value in a dictionary using the syntax: `dictionary[key]`.","metadata":{},"cell_type":"markdown","id":"9c5d9767-9bd5-4c5e-8a28-98a8c1eddc38"},{"source":"# Load JSON: json_data\nwith open(\"a_movie.json\") as json_file:\n    json_data = json.load(json_file)\n\n# Print each key-value pair in json_data\nfor k in json_data.keys():\n    print(k + ': ', json_data[k])","metadata":{},"cell_type":"code","id":"2ed9294a-f3aa-4b93-8a9a-8a23cadf5830","execution_count":null,"outputs":[]},{"source":"## 2. APIs and interacting with the world wide web\n\n## Herein, you’ll learn\n- What APIs are\n- Why APIs are important\n- In the exercises:\n    - Connecting to APIs\n    - Pulling data from APIs\n    - Parsing data from APIs\n\n## What is an API?\n\n- Set of protocols and routines\n- Bunch of code\n    - Allows two software programs to communicate with each other\n\n![Screen Shot 2023-08-29 at 1.30.17 PM](Screen%20Shot%202023-08-29%20at%201.30.17%20PM.png)\n\n## APIs are everywhere\n\n![Screen Shot 2023-08-29 at 1.31.01 PM](Screen%20Shot%202023-08-29%20at%201.31.01%20PM.png)\n","metadata":{},"cell_type":"markdown","id":"47cce4ea-40c1-4456-ad99-a14a65d8583f"},{"source":"## Connecting to an API in Python","metadata":{},"cell_type":"markdown","id":"258b6be8-0a3d-4c1a-8883-4e834e344b42"},{"source":"import requests\nurl = 'http://www.omdbapi.com/?t=hackers'\nr = requests.get(url)\njson_data = r.json()\nfor key, value in json_data.items():    \n    print(key + ':', value)\n","metadata":{},"cell_type":"code","id":"14eb70b7-0e5a-4f3a-bd08-125c749ead12","execution_count":null,"outputs":[]},{"source":"## What was that URL?\n- hp-making an HTTP request \n- www.omdbapi.com - queryingtheOMDBAPI\n- `?t=hackers`\n    - Query string\n    - Return data for a movie with title(t) ‘Hackers’`'http://www.omdbapi.com/?t=hackers'`","metadata":{},"cell_type":"markdown","id":"b7e5709b-4bea-4275-9f8c-f88a18975ac6"},{"source":"## API requests\nNow it's your turn to pull some movie data down from the Open Movie Database (OMDB) using their API. The movie you'll query the API about is The Social Network. Recall that, in the video, to query the API about the movie Hackers, Hugo's query string was 'http://www.omdbapi.com/?t=hackers' and had a single argument t=hackers.\n\nNote: recently, OMDB has changed their API: you now also have to specify an API key. This means you'll have to add another argument to the URL: apikey=72bc447a.\n\n### Instructions\n\n- Import the requests package.\n- Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding to the movie The Social Network. The query string should have two arguments: apikey=72bc447a and t=the+social+network. You can combine them as follows: apikey=72bc447a&t=the+social+network.\n- Print the text of the response object r by using its text attribute and passing the result to the print() function.","metadata":{},"cell_type":"markdown","id":"3cd17590-d09a-4b90-8afd-5f3478994505"},{"source":"# Import requests package\nimport requests\n\n# Assign URL to variable: url\nurl = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Print the text of the response\nprint(r.text)","metadata":{},"cell_type":"code","id":"1e18c957-466a-4eb6-9426-409f17fec633","execution_count":null,"outputs":[]},{"source":"## JSON–from the web to Python\nWow, congrats! You've just queried your first API programmatically in Python and printed the text of the response to the shell. However, as you know, your response is actually a JSON, so you can do one step better and decode the JSON. You can then print the key-value pairs of the resulting dictionary. That's what you're going to do now!\n\n### Instructions\n\n- Pass the variable url to the `requests.get()` function in order to send the relevant request and catch the response, assigning the resultant response message to the variable r.\n- Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.\n- Hit submit to print the key-value pairs of the dictionary json_data to the shell.","metadata":{},"cell_type":"markdown","id":"7ee35b64-5fec-460f-884e-bd0255eb8142"},{"source":"# Import package\nimport requests\n\n# Assign URL to variable: url\nurl = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Decode the JSON data into a dictionary: json_data\njson_data = r.json()\n\n# Print each key-value pair in json_data\nfor k in json_data.keys():\n    print(k + ': ', json_data[k])\n","metadata":{"executionCancelledAt":null,"executionTime":124,"lastExecutedAt":1693332003050,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import package\nimport requests\n\n# Assign URL to variable: url\nurl = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Decode the JSON data into a dictionary: json_data\njson_data = r.json()\n\n# Print each key-value pair in json_data\nfor k in json_data.keys():\n    print(k + ': ', json_data[k])\n","outputsMetadata":{"0":{"height":580,"type":"stream"}}},"cell_type":"code","id":"b330e758-cdb9-4cff-9b16-ec9dcb18197b","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Title:  The Social Network\nYear:  2010\nRated:  PG-13\nReleased:  01 Oct 2010\nRuntime:  120 min\nGenre:  Biography, Drama\nDirector:  David Fincher\nWriter:  Aaron Sorkin, Ben Mezrich\nActors:  Jesse Eisenberg, Andrew Garfield, Justin Timberlake\nPlot:  As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\nLanguage:  English, French\nCountry:  United States\nAwards:  Won 3 Oscars. 173 wins & 186 nominations total\nPoster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\nRatings:  [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\nMetascore:  95\nimdbRating:  7.8\nimdbVotes:  730,968\nimdbID:  tt1285016\nType:  movie\nDVD:  05 Jun 2012\nBoxOffice:  $96,962,694\nProduction:  N/A\nWebsite:  N/A\nResponse:  True\n"}]},{"source":"## Checking out the Wikipedia API\n\nYou're doing so well and having so much fun that we're going to throw one more API at you: the Wikipedia API (documented here). You'll figure out how to find and extract information from the Wikipedia page for Pizza. What gets a bit wild here is that your query will return nested JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into dictionaries within dictionaries.\n\nThe URL that requests the relevant query from the Wikipedia API is  \n\n`https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza`  \n### Instructions\n\n- Assign the relevant URL to the variable `url`.\n- Apply the `json()` method to the response object `r` and store the resulting dictionary in the variable `json_data`.\n- The variable `pizza_extract` holds the HTML of an extract from Wikipedia's Pizza page as a string; use the function `print()` to print this string to the shell.","metadata":{},"cell_type":"markdown","id":"2d6673dd-062a-40fd-a7bb-e0832c47ca85"},{"source":"# Import package\nimport requests\n\n# Assign URL to variable: url\nurl = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Decode the JSON data into a dictionary: json_data\njson_data = r.json()\n\n# Print the Wikipedia page extract\npizza_extract = json_data['query']['pages']['24768']['extract']\nprint(pizza_extract)","metadata":{"executionCancelledAt":null,"executionTime":279,"lastExecutedAt":1693331996412,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import package\nimport requests\n\n# Assign URL to variable: url\nurl = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n\n# Package the request, send the request and catch the response: r\nr = requests.get(url)\n\n# Decode the JSON data into a dictionary: json_data\njson_data = r.json()\n\n# Print the Wikipedia page extract\npizza_extract = json_data['query']['pages']['24768']['extract']\nprint(pizza_extract)","outputsMetadata":{"0":{"height":399,"type":"stream"}}},"cell_type":"code","id":"8c2b1c6e-ff4d-42d2-bc9b-26b25ffea038","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1033289096\">\n<p class=\"mw-empty-elt\">\n</p>\n<p><b>Pizza</b> (<span><small>English: </small></span> <i title=\"English pronunciation respelling\"><span>PEET</span>-sə</i>, <small>Italian: </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\" lang=\"it-Latn-fonipa\">[ˈpittsa]</span>, <small>Neapolitan: </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\" lang=\"nap-Latn-fonipa\">[ˈpittsə]</span>) is a dish of Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as various types of sausage, anchovies, mushrooms, onions, olives, vegetables, meat, ham, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven.</p><p>The term <i>pizza</i> was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania. Raffaele Esposito is often credited for creating modern pizza in Naples. In 2009, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.</p><p>Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food.  In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n</p><p>In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.  Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.</p>\n"}]},{"source":"# PART THREE","metadata":{},"cell_type":"markdown","id":"9321a05c-c7dd-4bb8-970d-3eaf7ad3025b"},{"source":"## 1 The Twitter API and Authentication\n\n## Herein, you’ll learn\n- How to stream data from the Twitter API\n- How to filter incoming tweets for keywords\n- About API Authentication and OAuth\n- How to use the Tweepy Python package\n\n![Screen Shot 2023-08-29 at 7.01.43 PM](Screen%20Shot%202023-08-29%20at%207.01.43%20PM.png)\n\n## Access the Twitter API\n\n![Screen Shot 2023-08-29 at 7.02.02 PM](Screen%20Shot%202023-08-29%20at%207.02.02%20PM.png)\n\n![Screen Shot 2023-08-29 at 7.02.15 PM](Screen%20Shot%202023-08-29%20at%207.02.15%20PM.png)\n\n![Screen Shot 2023-08-29 at 7.02.24 PM](Screen%20Shot%202023-08-29%20at%207.02.24%20PM.png)","metadata":{},"cell_type":"markdown","id":"44b9abfe-3b24-4eae-961c-b89bd2fe06f6"},{"source":"## Twitter has a number of APIs\n\n![Screen Shot 2023-08-29 at 7.44.02 PM](Screen%20Shot%202023-08-29%20at%207.44.02%20PM.png)\n\n![Screen Shot 2023-08-30 at 10.53.27 AM](Screen%20Shot%202023-08-30%20at%2010.53.27%20AM.png)\n\n![Screen Shot 2023-08-30 at 10.53.41 AM](Screen%20Shot%202023-08-30%20at%2010.53.41%20AM.png)\n\n![Screen Shot 2023-08-30 at 10.53.54 AM](Screen%20Shot%202023-08-30%20at%2010.53.54%20AM.png)\n\n![Screen Shot 2023-08-30 at 10.54.01 AM](Screen%20Shot%202023-08-30%20at%2010.54.01%20AM.png)\n","metadata":{},"cell_type":"markdown","id":"838ad5ba-7e09-44ef-89e4-6f8280fb95dc"},{"source":"## Tweets are returned as JSONs\n\n![Screen Shot 2023-08-30 at 10.54.13 AM](Screen%20Shot%202023-08-30%20at%2010.54.13%20AM.png)\n\n![Screen Shot 2023-08-30 at 10.54.19 AM](Screen%20Shot%202023-08-30%20at%2010.54.19%20AM.png)\n","metadata":{},"cell_type":"markdown","id":"4fd1313d-c8b7-43d0-b5bb-95153b71b701"},{"source":"## Using Tweepy: Authentication handler \ntw_auth.py\n","metadata":{},"cell_type":"markdown","id":"2dacb1c1-4392-4336-9e8c-24b4f0762100"},{"source":"import tweepy, json\naccess_token = \"...\"\naccess_token_secret = \"...\"\nconsumer_key = \"...\"\nconsumer_secret = \"...\"\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n","metadata":{"executionCancelledAt":null,"executionTime":20,"lastExecutedAt":1693389572290,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import tweepy, json\naccess_token = \"...\"\naccess_token_secret = \"...\"\nconsumer_key = \"...\"\nconsumer_secret = \"...\"\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n"},"cell_type":"code","id":"3be4b687-735a-445a-a28a-c6e13dc1a770","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtweepy\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m access_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m access_token_secret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"],"ename":"ModuleNotFoundError","evalue":"No module named 'tweepy'"}]},{"source":"## Tweepy: define stream listener class\nst_class.py","metadata":{},"cell_type":"markdown","id":"3f6b1255-14da-43c4-bb08-6db62e8ec064"},{"source":"class MyStreamListener(tweepy.StreamListener):\n    def __init__(self, api=None):        \n        super(MyStreamListener, self).__init__()        \n        self.num_tweets = 0        \n        self.file = open(\"tweets.txt\", \"w\")\n        def on_status(self, status):        \n            tweet = status._json        \n            self.file.write(json.dumps(tweet) + '\\\\n')        \n            tweet_list.append(status)        \n            self.num_tweets += 1\n            if self.num_tweets < 100:\n                return True\n            else:\n                return False        \n            self.file.close()","metadata":{"executionCancelledAt":null,"executionTime":697,"lastExecutedAt":1693389565760,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class MyStreamListener(tweepy.StreamListener):\n    def __init__(self, api=None):        \n        super(MyStreamListener, self).__init__()        \n        self.num_tweets = 0        \n        self.file = open(\"tweets.txt\", \"w\")\n        def on_status(self, status):        \n            tweet = status._json        \n            self.file.write(json.dumps(tweet) + '\\\\n')        \n            tweet_list.append(status)        \n            self.num_tweets += 1\n            if self.num_tweets < 100:\n                return True\n            else:\n                return False        \n            self.file.close()"},"cell_type":"code","id":"023c85e5-26f3-4abc-80b8-c0cf2287292e","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyStreamListener\u001b[39;00m(\u001b[43mtweepy\u001b[49m\u001b[38;5;241m.\u001b[39mStreamListener):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):        \n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(MyStreamListener, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()        \n","\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"],"ename":"NameError","evalue":"name 'tweepy' is not defined"}]},{"source":"## Using Tweepy:streamtweets!!\ntweets.py","metadata":{},"cell_type":"markdown","id":"d41bdd1f-131b-4508-825e-285c51da55c2"},{"source":"# Create Streaming object and authenticate\nl = MyStreamListener()\nstream = tweepy.Stream(auth, l)\n# This line filters Twitter Streams to capture data by keywords:\nstream.filter(track=['apples', 'oranges'])\n","metadata":{},"cell_type":"code","id":"96ada567-106f-4c57-9671-afc2e4d0c2ec","execution_count":null,"outputs":[]},{"source":"## Streaming tweets\nIt's time to stream some tweets! Your task is to create the Streamobject and to filter tweets according to particular keywords. tweepy has been imported for you.\n\n### Instructions\n\n- Create your Stream object with the credentials given.\n- Filter your Stream variable for the keywords \"clinton\", \"trump\", \"sanders\", and \"cruz\".","metadata":{},"cell_type":"markdown","id":"ad078479-9ee5-40d2-bace-208b0411f8fd"},{"source":"# Store credentials in relevant variables\nconsumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\nconsumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\naccess_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\naccess_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n\n# Create your Stream object with credentials\nstream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n\n# Filter your Stream variable\nstream.filter(track=[\"clinton\", \"trump\", \"sanders\", \"cruz\"])","metadata":{"executionCancelledAt":null,"executionTime":673,"lastExecutedAt":1693345090437,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Store credentials in relevant variables\nconsumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\nconsumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\naccess_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\naccess_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n\n# Create your Stream object with credentials\nstream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n\n# Filter your Stream variable\nstream.filter(track=[\"clinton\", \"trump\", \"sanders\", \"cruz\"])"},"cell_type":"code","id":"effad8df-3bf7-488c-aeaf-045e37e9ed71","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m access_token_secret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create your Stream object with credentials\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[43mtweepy\u001b[49m\u001b[38;5;241m.\u001b[39mStream(consumer_key, consumer_secret, access_token, access_token_secret)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Filter your Stream variable\u001b[39;00m\n\u001b[1;32m     11\u001b[0m stream\u001b[38;5;241m.\u001b[39mfilter(track\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclinton\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrump\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msanders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcruz\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"],"ename":"NameError","evalue":"name 'tweepy' is not defined"}]},{"source":"## Load and explore your Twitter data\nNow that you've got your Twitter data sitting locally in a text file, it's time to explore it! This is what you'll do in the next few interactive exercises. In this exercise, you'll read the Twitter data into a list: tweets_data.\n\nBe aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).\n\n### Instructions\n\n- Assign the filename 'tweets.txt' to the variable tweets_data_path.\n- Initialize tweets_data as an empty list to store the tweets in.\n- Within the for loop initiated by for line in tweets_file:, load each tweet into a variable, tweet, using json.loads(), then append tweet to tweets_data using the append() method.\n- Hit submit and check out the keys of the first tweet dictionary printed to the shell.","metadata":{},"cell_type":"markdown","id":"3b5ddd69-495b-4bca-bbae-0ad1cd50bf2a"},{"source":"# Import package\nimport json\n\n# String of path to file: tweets_data_path\ntweets_data_path = 'tweets.txt'\n\n# Initialize empty list to store tweets: tweets_data\ntweets_data = []\n\n# Open connection to file\ntweets_file = open(tweets_data_path, \"r\")\n\n# Read in tweets and store in list: tweets_data\nfor line in tweets_file:\n    tweets = json.loads(line)\n    tweets_data.append(tweets)\n\n# Close connection to file\ntweets_file.close()\n\n# Print the keys of the first tweet dict\nprint(tweets_data[0].keys())\n","metadata":{},"cell_type":"code","id":"9f1a5cd3-a832-498d-bfc4-13874f709e64","execution_count":null,"outputs":[]},{"source":"## Twitter data to DataFrame\nNow you have the Twitter data in a list of dictionaries, tweets_data, where each dictionary corresponds to a single tweet. Next, you're going to extract the text and language of each tweet. The text in a tweet, t1, is stored as the value t1['text']; similarly, the language is stored in t1['lang']. Your task is to build a DataFrame in which each row is a tweet and the columns are 'text' and 'lang'.\n\n### Instructions\n\n- Use pd.DataFrame() to construct a DataFrame of tweet texts and languages; to do so, the first argument should be tweets_data, a list of dictionaries. The second argument to pd.DataFrame() is a list of the keys you wish to have as columns. Assign the result of the pd.DataFrame() call to df.\n- Print the head of the DataFrame.","metadata":{},"cell_type":"markdown","id":"3ba5084e-c700-4020-8397-b4945d40b74f"},{"source":"","metadata":{},"cell_type":"code","id":"1dccebef-b27c-4fa9-bc7b-04ffd7f8e795","execution_count":null,"outputs":[]},{"source":"A little bit of Twitter text analysis\nNow that you have your DataFrame of tweets set up, you're going to do a bit of text analysis to count how many tweets contain the words 'clinton', 'trump', 'sanders' and 'cruz'. In the pre-exercise code, we have defined the following function word_in_text(), which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).\n\nimport re\n\ndef word_in_text(word, text):\n    word = word.lower()\n    text = text.lower()\n    match = re.search(word, text)\n\n    if match:\n        return True\n    return False\nYou're going to iterate over the rows of the DataFrame and calculate how many tweets contain each of our keywords! The list of objects for each candidate has been initialized to 0.\n\nInstructions\n100 XP\nWithin the for loop for index, row in df.iterrows():, the code currently increases the value of clinton by 1 each time a tweet (text row) mentioning 'Clinton' is encountered; complete the code so that the same happens for trump, sanders and cruz.","metadata":{},"cell_type":"markdown","id":"716c0bb3-c882-4e61-a936-d6b35ac9b146"},{"source":"# Initialize list to store tweet counts\n[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n\n# Iterate through df, counting the number of tweets in which\n# each candidate is mentioned\nfor index, row in df.iterrows():\n    clinton += word_in_text('clinton', row['text'])\n    trump += word_in_text('trump', row['text'])\n    sanders += word_in_text('sanders', row['text'])\n    cruz += word_in_text('cruz', row['text'])\n","metadata":{},"cell_type":"code","id":"d81cc993-6e2f-461b-8dc6-23b4ecaa8e91","execution_count":null,"outputs":[]},{"source":"## Plotting your Twitter data\n\nNow that you have the number of tweets that each candidate was mentioned in, you can plot a bar chart of this data. You'll use the statistical data visualization library seaborn, which you may not have seen before, but we'll guide you through. You'll first import seaborn as sns. You'll then construct a barplot of the data using sns.barplot, passing it two arguments:\n\na list of labels and \na list containing the variables you wish to plot (clinton, trump and so on.)\nHopefully, you'll see that Trump was unreasonably represented! We have already run the previous exercise solutions in your environment.\n\n### Instructions\n\n- Import both `matplotlib.pyplot` and `seaborn` using the aliases `plt` and `sns`, respectively.\n- Complete the arguments of sns.barplot:\n- The first argument should be the list of labels to appear on the x-axis (created in the previous step).\n- The second argument should be a list of the variables you wish to plot, as produced in the previous exercise (i.e. a list containing clinton, trump, etc).","metadata":{},"cell_type":"markdown","id":"1b41d572-161f-4bec-b95d-28789cb4f88c"},{"source":"# Import packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seaborn style\nsns.set(color_codes=True)\n\n# Create a list of labels:cd\ncd = ['clinton', 'trump', 'sanders', 'cruz']\n\n# Plot the bar chart\nax = sns.barplot(cd ,[clinton, trump, sanders, cruz])\nax.set(ylabel=\"count\")\nplt.show()","metadata":{},"cell_type":"code","id":"380c39c1-92c0-4d46-8a3c-539d3d97306c","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}