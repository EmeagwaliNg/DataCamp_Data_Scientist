{"cells":[{"source":"# Introduction to preprocessing","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":"# PART ONE","metadata":{},"cell_type":"markdown","id":"9d0385ec-8f31-4ae5-877c-8ec11ef28e44"},{"source":"## 1 What is data preprocessing?\n\n- After exploratory data analysis and data cleaning\n- Preparing data for modeling\n- Example: transforming categorical features into numerical features (dummy variables)\n\n## Why preprocess?\n\n- Transform dataset so it's suitable for modeling\n- Improve model performance\n- Generate more reliable results\n\n![Screen Shot 2023-08-30 at 11.24.15 AM](Screen%20Shot%202023-08-30%20at%2011.24.15%20AM.png)\n","metadata":{},"cell_type":"markdown","id":"ebe9b8b0-9448-4200-8263-49d21a45a213"},{"source":"## Recap: exploring data with pandas","metadata":{},"cell_type":"markdown","id":"9bd65e0e-8afb-4ced-8412-974416771efc"},{"source":"import pandas as pd\nhiking = pd.read_json(\"datasets/hiking.json\")\nwine = pd.read_csv('datasets/wine_types.csv')\nrunning_times_5k = pd.read_csv('datasets/running.csv')\nprint(hiking.head())\nprint(hiking.info())\nprint(wine.describe())","metadata":{"executionCancelledAt":null,"executionTime":204,"lastExecutedAt":1693435412371,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nhiking = pd.read_json(\"datasets/hiking.json\")\nwine = pd.read_csv('datasets/wine_types.csv')\nrunning_times_5k = pd.read_csv('datasets/running.csv')\nprint(hiking.head())\nprint(hiking.info())\nprint(wine.describe())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"7f26dd6a-20aa-4d19-8d3f-b8f64fba1821","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"  Prop_ID                     Name  ... lat lon\n0    B057  Salt Marsh Nature Trail  ... NaN NaN\n1    B073                Lullwater  ... NaN NaN\n2    B073                  Midwood  ... NaN NaN\n3    B073                Peninsula  ... NaN NaN\n4    B073                Waterfall  ... NaN NaN\n\n[5 rows x 11 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 33 entries, 0 to 32\nData columns (total 11 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Prop_ID         33 non-null     object \n 1   Name            33 non-null     object \n 2   Location        33 non-null     object \n 3   Park_Name       33 non-null     object \n 4   Length          29 non-null     object \n 5   Difficulty      27 non-null     object \n 6   Other_Details   31 non-null     object \n 7   Accessible      33 non-null     object \n 8   Limited_Access  33 non-null     object \n 9   lat             0 non-null      float64\n 10  lon             0 non-null      float64\ndtypes: float64(2), object(9)\nmemory usage: 3.0+ KB\nNone\n             Type     Alcohol  ...  OD280/OD315 of diluted wines      Proline\ncount  178.000000  178.000000  ...                    178.000000   178.000000\nmean     1.938202   13.000618  ...                      2.611685   746.893258\nstd      0.775035    0.811827  ...                      0.709990   314.907474\nmin      1.000000   11.030000  ...                      1.270000   278.000000\n25%      1.000000   12.362500  ...                      1.937500   500.500000\n50%      2.000000   13.050000  ...                      2.780000   673.500000\n75%      3.000000   13.677500  ...                      3.170000   985.000000\nmax      3.000000   14.830000  ...                      4.000000  1680.000000\n\n[8 rows x 14 columns]\n"}]},{"source":"## Removing missing data","metadata":{},"cell_type":"markdown","id":"7d9801ce-b9b5-4cc6-af83-a1e6c1229223"},{"source":"print(df)\nprint(df.dropna())\nprint(df.drop([1, 2, 3]))\nprint(df.drop(\"A\", axis=1))\nprint(df.isna().sum())\nprint(df.dropna(subset=[\"B\"]))\nprint(df.dropna(thresh=2))","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1693411844621,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"5e4f94fe-55c8-4a83-be63-1420e3692692","execution_count":3,"outputs":[]},{"source":"## Exploring missing data\nYou've been given a dataset comprised of volunteer information from New York City, stored in the volunteer DataFrame. Explore the dataset using the plethora of methods and attributes pandas has to offer to answer the following question.\n\n### How many missing values are in the locality column?","metadata":{},"cell_type":"markdown","id":"53867d9d-0fd9-470b-ad5b-edf043fabcdb"},{"source":"volunteer = pd.read_csv('datasets/volunteer_opportunities.csv')\nprint(volunteer['locality'].isna().sum())","metadata":{"executionCancelledAt":null,"executionTime":28,"lastExecutedAt":1693435419419,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"volunteer = pd.read_csv('datasets/volunteer_opportunities.csv')\nprint(volunteer['locality'].isna().sum())","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"d30efacd-c075-4f24-bff0-45e34682ab5b","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"70\n"}]},{"source":"## Dropping missing data\nNow that you've explored the volunteer dataset and understand its structure and contents, it's time to begin dropping missing values.\n\nIn this exercise, you'll drop both columns and rows to create a subset of the volunteer dataset.\n\n### Instructions\n\n- Drop the `Latitude` and `Longitude` columns from `volunteer`, storing as volunteer_cols.\n- Subset `volunteer_cols` by dropping rows containing missing values in the `category_desc`, and store in a new variable called `volunteer_subset`.\n- Take a look at the `.shape` attribute of `volunteer_subset`, to verify it worked correctly.","metadata":{},"cell_type":"markdown","id":"a902f6c4-d578-4ac5-9b3e-565654f97567"},{"source":"# Drop the Latitude and Longitude columns from volunteer\nvolunteer_cols = volunteer.drop(['Latitude', 'Longitude'], axis =1)\n\n# Drop rows with missing category_desc values from volunteer_cols\nvolunteer_subset = volunteer_cols.dropna(subset=['category_desc'])\n\n# Print out the shape of the subset\nprint(volunteer_subset.shape)","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1693435429883,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Drop the Latitude and Longitude columns from volunteer\nvolunteer_cols = volunteer.drop(['Latitude', 'Longitude'], axis =1)\n\n# Drop rows with missing category_desc values from volunteer_cols\nvolunteer_subset = volunteer_cols.dropna(subset=['category_desc'])\n\n# Print out the shape of the subset\nprint(volunteer_subset.shape)","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"50967aca-47be-46c9-b863-4da936a049d9","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"(617, 33)\n"}]},{"source":"## 2 Working With Data Types\n\n## Why are types important?\n\n- `object`: string/mixed types\n- `int64`: integer\n- `float64`: float\n- `datetime64`: dates and times","metadata":{},"cell_type":"markdown","id":"d4b5065d-f60c-40ba-8477-db0632786cf2"},{"source":"## Converting column types","metadata":{},"cell_type":"markdown","id":"514707c9-5e7e-4d81-a3a5-88c51a324e9d"},{"source":"print(df.info())\ndf[\"C\"] = df[\"C\"].astype(\"float\")\nprint(df.dtypes)","metadata":{"executionCancelledAt":1693411809020},"cell_type":"code","id":"223030e5-3b36-459f-b485-fb64420e6083","execution_count":null,"outputs":[]},{"source":"## Exploring data types\nTaking another look at the dataset comprised of volunteer information from New York City, you want to know what types you'll be working with as you start to do more preprocessing.\n\n### Which data types are present in the volunteer dataset?","metadata":{},"cell_type":"markdown","id":"12f4a03f-3039-4cd1-8ad8-f6cd26cf3ef6"},{"source":"print(volunteer.info())","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1693411866810,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(volunteer.info())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"02e1b27f-b65e-4b75-8c3e-3eaac0524ba0","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 665 entries, 0 to 664\nData columns (total 35 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   opportunity_id      665 non-null    int64  \n 1   content_id          665 non-null    int64  \n 2   vol_requests        665 non-null    int64  \n 3   event_time          665 non-null    int64  \n 4   title               665 non-null    object \n 5   hits                665 non-null    int64  \n 6   summary             665 non-null    object \n 7   is_priority         62 non-null     object \n 8   category_id         617 non-null    float64\n 9   category_desc       617 non-null    object \n 10  amsl                0 non-null      float64\n 11  amsl_unit           0 non-null      float64\n 12  org_title           665 non-null    object \n 13  org_content_id      665 non-null    int64  \n 14  addresses_count     665 non-null    int64  \n 15  locality            595 non-null    object \n 16  region              665 non-null    object \n 17  postalcode          659 non-null    float64\n 18  primary_loc         0 non-null      float64\n 19  display_url         665 non-null    object \n 20  recurrence_type     665 non-null    object \n 21  hours               665 non-null    int64  \n 22  created_date        665 non-null    object \n 23  last_modified_date  665 non-null    object \n 24  start_date_date     665 non-null    object \n 25  end_date_date       665 non-null    object \n 26  status              665 non-null    object \n 27  Latitude            0 non-null      float64\n 28  Longitude           0 non-null      float64\n 29  Community Board     0 non-null      float64\n 30  Community Council   0 non-null      float64\n 31  Census Tract        0 non-null      float64\n 32  BIN                 0 non-null      float64\n 33  BBL                 0 non-null      float64\n 34  NTA                 0 non-null      float64\ndtypes: float64(13), int64(8), object(14)\nmemory usage: 182.0+ KB\nNone\n"}]},{"source":"## Converting a column type\n\nIf you take a look at the volunteer dataset types, you'll see that the column hits is type object. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type int.\n\n### Instructions\n\n- Take a look at the `.head()` of the hits column.\n- Convert the `hits` column to type `int`.\n- Take a look at the `.dtypes` of the dataset again, and notice that the column type has changed.","metadata":{},"cell_type":"markdown","id":"e281a388-100d-4d85-a108-ed3e79e06e86"},{"source":"# Print the head of the hits column\nprint(volunteer[\"hits\"].head())\n\n# Convert the hits column to type int\nvolunteer[\"hits\"] = volunteer[\"hits\"].astype('int')\n\n# Look at the dtypes of the dataset\nprint(volunteer.dtypes)","metadata":{"executionCancelledAt":null,"executionTime":18,"lastExecutedAt":1693435438664,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print the head of the hits column\nprint(volunteer[\"hits\"].head())\n\n# Convert the hits column to type int\nvolunteer[\"hits\"] = volunteer[\"hits\"].astype('int')\n\n# Look at the dtypes of the dataset\nprint(volunteer.dtypes)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"4cc65ec3-8067-43f0-ac08-14827a95d747","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"0    737\n1     22\n2     62\n3     14\n4     31\nName: hits, dtype: int64\nopportunity_id          int64\ncontent_id              int64\nvol_requests            int64\nevent_time              int64\ntitle                  object\nhits                    int64\nsummary                object\nis_priority            object\ncategory_id           float64\ncategory_desc          object\namsl                  float64\namsl_unit             float64\norg_title              object\norg_content_id          int64\naddresses_count         int64\nlocality               object\nregion                 object\npostalcode            float64\nprimary_loc           float64\ndisplay_url            object\nrecurrence_type        object\nhours                   int64\ncreated_date           object\nlast_modified_date     object\nstart_date_date        object\nend_date_date          object\nstatus                 object\nLatitude              float64\nLongitude             float64\nCommunity Board       float64\nCommunity Council     float64\nCensus Tract          float64\nBIN                   float64\nBBL                   float64\nNTA                   float64\ndtype: object\n"}]},{"source":"## 3 Training and testsets\n\n## Why split?\n1. Reduces overfitting\n2. Evaluate performance on a holdout set","metadata":{},"cell_type":"markdown","id":"cdf440b9-8a1b-4fa4-a110-cdcbc8452698"},{"source":"## Splitting up your dataset","metadata":{},"cell_type":"markdown","id":"bee861ad-4798-4764-9410-455c8b4bc9c9"},{"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"executionCancelledAt":1693411809023},"cell_type":"code","id":"6c548db8-ce31-4e64-b487-c5d0945abf65","execution_count":null,"outputs":[]},{"source":"## Stratified sampling\n- Dataset of 100 samples: 80 **class** 1 and 20 **class** 2\n- Training set of 75 samples: 60 **class** 1 and 15 **class** 2\n- Test set of 25 samples: 20 **class** 1 and 5 **class** 2","metadata":{},"cell_type":"markdown","id":"0ebf6e34-9cb5-4a1a-8cd0-953a1aa189c5"},{"source":"## Stratified sampling","metadata":{},"cell_type":"markdown","id":"3be77a6e-c105-4b72-865b-fb85ab3dfc27"},{"source":"X_train,X_test,y_train,y_test = train_test_split(X, y, stratify=y, random_state=42)\ny[\"labels\"].value_counts()\ny_train[\"labels\"].value_counts()\ny_test[\"labels\"].value_counts()","metadata":{"executionCancelledAt":1693411809023},"cell_type":"code","id":"5769faee-f618-4fc8-a9fe-19e690e95423","execution_count":null,"outputs":[]},{"source":"## Exercise\nIn the volunteer dataset, you're thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, you need to know what the class distribution (and imbalance) is for that label.\n\n## Which descriptions occur less than 50 times in the volunteer dataset?","metadata":{},"cell_type":"markdown","id":"744cb25d-c440-4af1-ba65-adbed80e8948"},{"source":"classes = volunteer['category_desc'].value_counts()\nprint(classes)\nprint(classes[classes <50])","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1693411883472,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"classes = volunteer['category_desc'].value_counts()\nprint(classes)\nprint(classes[classes <50])","outputsMetadata":{"0":{"height":218,"type":"stream"}}},"cell_type":"code","id":"1389562c-c3f3-4170-be80-edc914b64e44","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"Strengthening Communities    307\nHelping Neighbors in Need    119\nEducation                     92\nHealth                        52\nEnvironment                   32\nEmergency Preparedness        15\nName: category_desc, dtype: int64\nEnvironment               32\nEmergency Preparedness    15\nName: category_desc, dtype: int64\n"}]},{"source":"## Stratified sampling\nYou now know that the distribution of class labels in the category_desc column of the volunteer dataset is uneven. If you wanted to train a model to predict category_desc, you'll need to ensure that the model is trained on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this!\n\n### Instructions\n\n- Create a DataFrame of features, `X`, with all of the columns except `category_desc`.\n- Create a DataFrame of labels, `y` from the category_desc column.\n- Split `X` and `y` into training and test sets, ensuring that the class distribution in the labels is the same in both sets\n- Print the labels and counts in `y_train` using `.value_counts()`.","metadata":{},"cell_type":"markdown","id":"a90c87df-de95-4b06-91f8-cc24b5938eb8"},{"source":"# import package\nfrom sklearn.model_selection import train_test_split\nvolunteer = pd.read_csv('datasets/volunteer.csv')\n# Create a DataFrame with all columns except category_desc\nX = volunteer.drop('category_desc', axis=1)\n\n# Create a category_desc labels dataset\ny = volunteer[['category_desc']]\n\n# Use stratified sampling to split up the dataset according to the y dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Print the category_desc counts from y_train\nprint(y_train['category_desc'].value_counts())","metadata":{"executionCancelledAt":null,"executionTime":434,"lastExecutedAt":1693411890242,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# import package\nfrom sklearn.model_selection import train_test_split\nvolunteer = pd.read_csv('datasets/volunteer.csv')\n# Create a DataFrame with all columns except category_desc\nX = volunteer.drop('category_desc', axis=1)\n\n# Create a category_desc labels dataset\ny = volunteer[['category_desc']]\n\n# Use stratified sampling to split up the dataset according to the y dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Print the category_desc counts from y_train\nprint(y_train['category_desc'].value_counts())","outputsMetadata":{"0":{"height":158,"type":"stream"}}},"cell_type":"code","id":"1b7f1717-0155-4e50-be30-dfa50d5750ee","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"Strengthening Communities    230\nHelping Neighbors in Need     89\nEducation                     69\nHealth                        39\nEnvironment                   24\nEmergency Preparedness        11\nName: category_desc, dtype: int64\n"}]},{"source":"# PART TWO","metadata":{},"cell_type":"markdown","id":"ce376593-55dc-43ee-95d3-4fee781f011c"},{"source":"## 1 Standardization\n\n## What is standardization?\n\n**Standardization**: transform continuous data to appear normally distributed  \n- scikit-learn models assume normally distributed data\n- Using non-normal training data can introduce bias\n- Log normalization and feature scaling in this course\n- Applied to continuous numerical data\n\n\n## When to standardize: linear distances\n- Model in linear space\n- Examples:\n    - k-Nearest Neighbors (kNN)\n    - Linear regression\n    - K-Means Clustering\n\n![Screen Shot 2023-08-30 at 12.40.26 PM](Screen%20Shot%202023-08-30%20at%2012.40.26%20PM.png)\n\n## When to standardize: high variance\n- Model in linear space\n\n- Examples:\n    - k-Nearest Neighbors (kNN)\n    - Linear regression\n    - K-Means Clustering\n\n- Dataset features have high variance\n\n## When to standardize: different scales\n\n- Features are on different scales\n- Example:\n    - Predicting house prices using no. bedrooms and last sale price\n- Linearity assumptions\n","metadata":{},"cell_type":"markdown","id":"a8caedca-35d7-46bb-8e82-efd2cdcbcb3f"},{"source":"## Modeling without normalizing\nLet's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first.\n\nHere we have a subset of the `wine` dataset. One of the columns, `Proline`, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n\nThe `scikit-learn` model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the `X` and `y` sets you need to fit and score on.\n\n### Instructions\n\n- Split up the `X` and `y` sets into training and test sets, ensuring that class labels are equally distributed in both sets.\n- Fit the `knn` model to the training features and labels.\n- Print the test set accuracy of the `knn` model using the `.score()` method.","metadata":{},"cell_type":"markdown","id":"bf054be2-a9be-4f02-855a-2e9f12f77788"},{"source":"# Import the datasets\nX = pd.read_csv('datasets/X.csv')\ny = pd.read_csv('datasets/y.csv', index_col=[0])\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n# Import KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\n# Fit the knn model to the training data\nknn.fit(X_train, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test, y_test))","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1693411902015,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the datasets\nX = pd.read_csv('datasets/X.csv')\ny = pd.read_csv('datasets/y.csv', index_col=[0])\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n# Import KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\n# Fit the knn model to the training data\nknn.fit(X_train, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test, y_test))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"3cdbbe02-ee7e-4a29-b07c-94a025be9f67","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"0.9777777777777777\n"}]},{"source":"## 2 Log normalization\n\n## What is log normalization?\n\n- Useful for features with high variance\n- Applies logarithm transformation\n- Natural log using the constant $\\epsilon(\\approx 2.718)$\n- $\\epsilon ^{3.4} = 30$\n- Captures relative changes, the magnitudeof change, and keeps everything positive\n\n|Number  |Log|\n|----  |---|\n|30  |3.4|\n|300  |5.7|\n|3000  |8|\n","metadata":{},"cell_type":"markdown","id":"3ed59105-59be-4353-a00d-a6ac1424ecd4"},{"source":"## Log normalization in Python","metadata":{},"cell_type":"markdown","id":"9797d0aa-05dc-48dc-ac42-bc62159926d2"},{"source":"print(df)\nimport numpy as np\ndf[\"log_2\"] = np.log(df[\"col2\"])\nprint(df)\nprint(df[[\"col1\", \"log_2\"]].var())","metadata":{"executionCancelledAt":1693411809028},"cell_type":"code","id":"8e41429e-fc9b-4c82-920b-11fdcee32c95","execution_count":null,"outputs":[]},{"source":"## Checking the variance\nCheck the variance of the columns in the wine dataset.  \nOut of the four columns listed, which column is the most appropriate candidate for normalization?","metadata":{},"cell_type":"markdown","id":"629dd414-eec1-4ba8-b3c3-0da70502fa50"},{"source":"print(wine.var())","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1693411910019,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(wine.var())","outputsMetadata":{"0":{"height":319,"type":"stream"}}},"cell_type":"code","id":"7d6ec293-ac79-4e5d-b51c-d1d6ea74d6ef","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"Type                                0.600679\nAlcohol                             0.659062\nMalic acid                          1.248015\nAsh                                 0.075265\nAlcalinity of ash                  11.152686\nMagnesium                         203.989335\nTotal phenols                       0.391690\nFlavanoids                          0.997719\nNonflavanoid phenols                0.015489\nProanthocyanins                     0.327595\nColor intensity                     5.374449\nHue                                 0.052245\nOD280/OD315 of diluted wines        0.504086\nProline                         99166.717355\ndtype: float64\n"}]},{"source":"## Log normalization in Python\n\nNow that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.\n`numpy` has been imported as `np`.\n\n### Instructions\n\n- Print out the variance of the Proline column for reference.\n- Use the `np.log()` function on the Proline column to create a new, log-normalized column named `Proline_log`.\n- Print out the variance of the `Proline_log` column to see the difference.","metadata":{},"cell_type":"markdown","id":"27e3c8d5-f649-4f3c-b5ba-c42334768e3f"},{"source":"# Import numpy as np\nimport numpy as np\n# Print out the variance of the Proline column\nprint(wine['Proline'].var())\n\n# Apply the log normalization function to the Proline column\nwine['Proline_log'] = np.log(wine['Proline'])\n\n# Check the variance of the normalized Proline column\nprint(wine['Proline_log'].var())","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1693411938737,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import numpy as np\nimport numpy as np\n# Print out the variance of the Proline column\nprint(wine['Proline'].var())\n\n# Apply the log normalization function to the Proline column\nwine['Proline_log'] = np.log(wine['Proline'])\n\n# Check the variance of the normalized Proline column\nprint(wine['Proline_log'].var())","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"cell_type":"code","id":"d51838e4-6edc-4361-82db-62817566a0ac","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"99166.71735542436\n0.17231366191842012\n"}]},{"source":"## 3 Scaling data\n\n## What is feature scaling?\n\n- Features on different scales\n- Model with linear characteristics\n- Center features around 0 and transform to variance of 1\n- Transforms to approximately normal distribution\n","metadata":{},"cell_type":"markdown","id":"fccfc56e-9e90-4880-ac5a-52e71aee8189"},{"source":"## How to scale data","metadata":{},"cell_type":"markdown","id":"c484e73d-0e0c-4565-86ef-c71ba7f54a47"},{"source":"print(df)\nprint(df.var())\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\nprint(df_scaled)\nprint(df_scaled.var())","metadata":{"executionCancelledAt":1693411809030},"cell_type":"code","id":"e10191d5-2e3e-4bd4-ad4c-40157c222e1b","execution_count":null,"outputs":[]},{"source":"## Scaling data - investigating columns\nYou want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model.\n\n### Which of the following statements about these columns is true?","metadata":{},"cell_type":"markdown","id":"6fdc8635-5383-4b3b-9850-17a1ccb259e1"},{"source":"print(wine.describe())","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1693411944881,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(wine.describe())","outputsMetadata":{"0":{"height":238,"type":"stream"}}},"cell_type":"code","id":"e834834e-68e9-4d0c-a5df-35c9837684fa","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"             Type     Alcohol  ...      Proline  Proline_log\ncount  178.000000  178.000000  ...   178.000000   178.000000\nmean     1.938202   13.000618  ...   746.893258     6.530303\nstd      0.775035    0.811827  ...   314.907474     0.415107\nmin      1.000000   11.030000  ...   278.000000     5.627621\n25%      1.000000   12.362500  ...   500.500000     6.215606\n50%      2.000000   13.050000  ...   673.500000     6.512486\n75%      3.000000   13.677500  ...   985.000000     6.892642\nmax      3.000000   14.830000  ...  1680.000000     7.426549\n\n[8 rows x 15 columns]\n"}]},{"source":"## Scaling data - standardizing columns\nSince we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.\n\n### Instructions\n\n- Import the StandardScaler class.\n- Instantiate a StandardScaler() and store it in the variable, scaler.\n- Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset.\n- Fit and transform the standard scaler to wine_subset.","metadata":{},"cell_type":"markdown","id":"597f1cc9-f1f9-42df-bfa5-fc99942125b3"},{"source":"# Import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Create the scaler\nscaler = StandardScaler()\n\n# Subset the DataFrame you want to scale \nwine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n\n# Apply the scaler to wine_subset\nwine_subset_scaled = scaler.fit_transform(wine_subset)","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1693411952749,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Create the scaler\nscaler = StandardScaler()\n\n# Subset the DataFrame you want to scale \nwine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n\n# Apply the scaler to wine_subset\nwine_subset_scaled = scaler.fit_transform(wine_subset)"},"cell_type":"code","id":"1f6e5353-d8d5-4e1d-b1a8-6775c45e12b5","execution_count":16,"outputs":[]},{"source":"## 4 Standardized data and modeling\n\n## K-nearest neighbors\n- Data leakage: non-training data is used to train the model","metadata":{},"cell_type":"markdown","id":"0fc504c1-0f1e-41c1-965b-dc26719a927f"},{"source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\nknn = KNeighborsClassifier()\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nknn.fit(X_train_scaled, y_train)\nknn.score(X_test_scaled, y_test)","metadata":{"executionCancelledAt":1693411809033},"cell_type":"code","id":"902a7528-4102-4895-8f15-b49b5bed12ce","execution_count":null,"outputs":[]},{"source":"## KNN on non-scaled data\n\nBefore adding standardization to your `scikit-learn` workflow, you'll first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.\n\nThe knn model as well as the` X` and `y` data and labels sets have been created already.\n\n### Instructions\n\n- Split the dataset into training and test sets.\n- Fit the `knn` model to the training data.\n- Print out the test set accuracy of your trained `knn` model.","metadata":{},"cell_type":"markdown","id":"2e00a81e-7d2c-4efe-a76f-a669e407a115"},{"source":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Fit the k-nearest neighbors model to the training data\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test, y_test))","metadata":{"executionCancelledAt":null,"executionTime":20,"lastExecutedAt":1693411960688,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Fit the k-nearest neighbors model to the training data\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test, y_test))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"ae1c8619-6592-4192-ade8-3e3f84f4240c","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"0.9777777777777777\n"}]},{"source":"## KNN on scaled data\nThe accuracy score on the unscaled wine dataset was decent, but let's see what you can achieve by using standardization. Once again, the knn model as well as the X and y data and labels set have already been created for you.\n\n### Instructions\n\n- Create the `StandardScaler()` method, stored in a variable named `scaler`.\n- Scale the training and test features, being careful not to introduce data leakage.\n- Fit the `knn` model to the scaled training data.\n- Evaluate the model's performance by computing the test set accuracy.","metadata":{},"cell_type":"markdown","id":"f3320157-3945-4a24-b0d6-396588961536"},{"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Instantiate a StandardScaler\nscaler = StandardScaler()\n\n# Scale the training and test features\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Fit the k-nearest neighbors model to the training data\nknn.fit(X_train_scaled, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test_scaled, y_test))","metadata":{"executionCancelledAt":null,"executionTime":21,"lastExecutedAt":1693411966298,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Instantiate a StandardScaler\nscaler = StandardScaler()\n\n# Scale the training and test features\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Fit the k-nearest neighbors model to the training data\nknn.fit(X_train_scaled, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test_scaled, y_test))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"e5188250-171a-4020-bc26-063aba252caf","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"0.9777777777777777\n"}]},{"source":"# PART THREE","metadata":{},"cell_type":"markdown","id":"c6191206-769c-4972-bc63-cb093e7863d8"},{"source":"## 1 Feature engineering\n\n## What is feature engineering?\n\nFeature engineering: Creation of new features from existing ones\n- Improve performance\n- Insight into relationships between features\n- Need to understand the data first!\n- Highly dataset-dependent\n\n## Feature engineering scenarios\n\n|Id  |Text|\n|--  |---|\n|1  |\"Feature engineering is fun!\"|\n|2  |\"Feature engineering is a lot of work.\"|\n|3  |\"I don't mind feature engineering.\"|\n\n|user  |fav_color|\n|---   |------|\n|1  |blue|\n|2  |green|\n|3  |orange|\n\n\n|Id  |Date|\n|---  |---| \n|4  |July 30 2011|\n|5  |January 29 2011|\n|6  |February 05 2011|\n\n|user  |test1  |test2  |test3|\n|--|--|--|--|\n|1  |90.5  |89.6  |91.4|\n|2  |65.5  |70.6  |67.3|\n|3  |78.1  |80.7  |81.8|\n","metadata":{},"cell_type":"markdown","id":"359e3287-7f23-44de-bbdb-7f038f75c838"},{"source":"## 2 Encoding categorical variables\n\n## Categorical variables\n\n|  |user  |subscribed  |fav_color|\n|--|--|--|--|\n|0     |1          |y      |blue|\n|1     |2          |n     |green|\n|2     |3          |n    |orange|\n|3     |4          |y     |green|\n","metadata":{},"cell_type":"markdown","id":"d5161b63-0ffe-4fbf-8437-a049c83443a3"},{"source":"## Encoding binary variables - pandas","metadata":{},"cell_type":"markdown","id":"a448958a-5c5d-4556-b750-eae90e5c7693"},{"source":"print(users[\"subscribed\"])\nusers[\"sub_enc\"] = users[\"subscribed\"].apply(lambda val: 1 if val == \"y\" else 0)\nprint(users[[\"subscribed\", \"sub_enc\"]])","metadata":{"executionCancelledAt":1693411809035},"cell_type":"code","id":"174da348-6cff-445c-b1cd-832e87bd31eb","execution_count":null,"outputs":[]},{"source":"## Encoding binary variables - scikit-learn","metadata":{},"cell_type":"markdown","id":"ffbedc2b-9206-40e9-a3b4-2ff1744d2a0b"},{"source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nusers[\"sub_enc_le\"] = le.fit_transform(users[\"subscribed\"])\nprint(users[[\"subscribed\", \"sub_enc_le\"]])","metadata":{"executionCancelledAt":1693411809035},"cell_type":"code","id":"c1cadcc0-a2ab-4ced-887f-6fd86f91d019","execution_count":null,"outputs":[]},{"source":"## Encoding categorical variables - binary\n\nTake a look at the hiking dataset. There are several columns here that need encoding before they can be modeled, one of which is the Accessible column. Accessible is a binary feature, so it has two values, Y or N, which need to be encoded into 1's and 0's. Use `scikit-learn`'s LabelEncoder method to perform this transformation.\n\n### Instructions\n\n- Store `LabelEncoder()` in a variable named enc.\n- Using the encoder's `.fit_transform()` method, encode the hiking dataset's `\"Accessible\"` column. Call the new column `Accessible_enc`.\n- Compare the two columns side-by-side to see the encoding.","metadata":{},"cell_type":"markdown","id":"7cdcc8cc-7f54-477c-8a4f-af884ba10a86"},{"source":"from sklearn.preprocessing import LabelEncoder\n# Set up the LabelEncoder object\nenc = LabelEncoder()\n\n# Apply the encoding to the \"Accessible\" column\nhiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n\n# Compare the two columns\nprint(hiking[[\"Accessible\", \"Accessible_enc\"]].head())","metadata":{"executionCancelledAt":null,"executionTime":17,"lastExecutedAt":1693411978238,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.preprocessing import LabelEncoder\n# Set up the LabelEncoder object\nenc = LabelEncoder()\n\n# Apply the encoding to the \"Accessible\" column\nhiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n\n# Compare the two columns\nprint(hiking[[\"Accessible\", \"Accessible_enc\"]].head())","outputsMetadata":{"0":{"height":138,"type":"stream"}}},"cell_type":"code","id":"c3f1ab0c-f100-4a85-8870-69d768933344","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"  Accessible  Accessible_enc\n0          Y               1\n1          N               0\n2          N               0\n3          N               0\n4          N               0\n"}]},{"source":"## One-hot encoding\n\nValues: [blue, green, orange]\n\n- blue: [1, 0, 0]\n- green: [0, 1, 0]\n- orange: [0, 0, 1]","metadata":{},"cell_type":"markdown","id":"b99eb57a-d32a-4c3e-b450-1ca44466bda9"},{"source":"print(users[\"fav_color\"])\nprint(pd.get_dummies(users[\"fav_color\"]))","metadata":{"executionCancelledAt":1693411809037},"cell_type":"code","id":"1666289e-64d6-4be7-ba44-06ea9d4c2bba","execution_count":null,"outputs":[]},{"source":"## Encoding categorical variables - one-hot\n\nOne of the columns in the volunteer dataset, `category_desc`, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use pandas' `pd.get_dummies()` function to do so.\n\n### Instructions\n\n- Call get_dummies() on the `volunteer[\"category_desc\"]` column to create the encoded columns and assign it to `category_enc`.\n- Print out the `.head()` of the `category_enc` variable to take a look at the encoded columns.","metadata":{},"cell_type":"markdown","id":"489f0210-d72c-49c4-9185-0b89b1f67e1c"},{"source":"# Transform the category_desc column\ncategory_enc = pd.get_dummies(volunteer[\"category_desc\"])\n\n# Take a look at the encoded columns\nprint(category_enc.head())","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1693411985322,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Transform the category_desc column\ncategory_enc = pd.get_dummies(volunteer[\"category_desc\"])\n\n# Take a look at the encoded columns\nprint(category_enc.head())","outputsMetadata":{"0":{"height":178,"type":"stream"}}},"cell_type":"code","id":"605f3935-6b01-4ccb-9f71-dc831f0ec848","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"   Education  ...  Strengthening Communities\n0          0  ...                          1\n1          0  ...                          1\n2          0  ...                          1\n3          0  ...                          0\n4          0  ...                          0\n\n[5 rows x 6 columns]\n"}]},{"source":"## 3 Engineering numerical features","metadata":{},"cell_type":"markdown","id":"45985f0f-0d96-4451-831d-5236d0f9c25f"},{"source":"print(temps)\ntemps[\"mean\"] = temps.loc[:,\"day1\":\"day3\"].mean(axis=1)\nprint(temps)","metadata":{"executionCancelledAt":1693411809038},"cell_type":"code","id":"1e26b8fe-1904-4533-b99f-94d8fdc985ed","execution_count":null,"outputs":[]},{"source":"## Dates","metadata":{},"cell_type":"markdown","id":"0398030c-2892-4a15-a6a4-fe9369dae52b"},{"source":"print(purchases)\npurchases[\"date_converted\"] = pd.to_datetime(purchases[\"date\"])\npurchases['month'] = purchases[\"date_converted\"].dt.month\nprint(purchases)","metadata":{"executionCancelledAt":1693411809038},"cell_type":"code","id":"c8b9e581-ca42-42d6-bf11-10f843a05556","execution_count":null,"outputs":[]},{"source":"## Aggregating numerical features\nA good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named `running_times_5k`. For each name in the dataset, take the mean of their 5 run times.\n\n### Instructions\n\n- Use the .`loc[]` method to select all rows and columns to find the `.mean()` of the each columns.\n- Print the `.head()` of the DataFrame to see the mean column.\n\n","metadata":{},"cell_type":"markdown","id":"23a23aab-fbfb-423b-aec2-f26597666c6b"},{"source":"# Use .loc to create a mean column\nrunning_times_5k[\"mean\"] = running_times_5k.loc[:, running_times_5k.columns].mean(axis=1)\n\n# Take a look at the results\nprint(running_times_5k.head())","metadata":{"executionCancelledAt":null,"executionTime":21,"lastExecutedAt":1693411994942,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Use .loc to create a mean column\nrunning_times_5k[\"mean\"] = running_times_5k.loc[:, running_times_5k.columns].mean(axis=1)\n\n# Take a look at the results\nprint(running_times_5k.head())","outputsMetadata":{"0":{"height":138,"type":"stream"}}},"cell_type":"code","id":"bafeb1d4-863b-431a-869c-b82630e5237a","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"   Unnamed: 0   name  run1  run2  run3  run4  run5       mean\n0           0    Sue  20.1  18.5  19.6  20.3  18.3  16.133333\n1           1   Mark  16.5  17.1  16.9  17.6  17.3  14.400000\n2           2   Sean  23.5  25.1  25.2  24.6  23.9  20.716667\n3           3   Erin  21.7  21.1  20.9  22.1  22.2  18.500000\n4           4  Jenny  25.8  27.1  26.1  26.7  26.9  22.766667\n"}]},{"source":"## Extracting datetime components\n\nThere are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the `start_date_date` column and extract just the month to use as a feature for modeling.\n\n### Instructions\n\n- Convert the `start_date_date` column into a `pandas` datetime column and store it in a new column called `start_date_converted`.\n- Retrieve the month component of `start_date_converted` and store it in a new column called `start_date_month`.\n- Print the `.head()` of just the `start_date_converted` and `start_date_month` columns.","metadata":{},"cell_type":"markdown","id":"161c9986-77ee-42df-97b1-5b7c222a9fd7"},{"source":"# First, convert string column to date column\nvolunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n\n# Extract just the month from the converted column\nvolunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n\n# Take a look at the converted and new month columns\nprint(volunteer[['start_date_converted', 'start_date_month']].head())","metadata":{"executionCancelledAt":null,"executionTime":66,"lastExecutedAt":1693412000779,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# First, convert string column to date column\nvolunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n\n# Extract just the month from the converted column\nvolunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n\n# Take a look at the converted and new month columns\nprint(volunteer[['start_date_converted', 'start_date_month']].head())","outputsMetadata":{"0":{"height":138,"type":"stream"}}},"cell_type":"code","id":"3d36bd57-f67d-4aee-84bb-610aa76f42d0","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"  start_date_converted  start_date_month\n0           2011-02-01                 2\n1           2011-01-29                 1\n2           2011-02-14                 2\n3           2011-02-05                 2\n4           2011-02-12                 2\n"}]},{"source":"## 4 Engineering text features\n\n## Extraction\nRegular expressions: \ncode to identify patterns\n\n- `\\d+`: \\d means we want to grab digits, `+` means we want to grab as many as possible\n- `\\.` : `\\.` means we want to grab periods/decimal points\n- `\\d+`: Putting this after the decimal point means grab the ditis after the point.","metadata":{},"cell_type":"markdown","id":"686e9da5-fe26-459a-b3bf-c022eb58ed34"},{"source":"import re\nmy_string = \"temperature:75.6 F\"\ntemp = re.search(\"\\d+\\.\\d+\", my_string)\nprint(float(temp.group(0)))","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1693412007616,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import re\nmy_string = \"temperature:75.6 F\"\ntemp = re.search(\"\\d+\\.\\d+\", my_string)\nprint(float(temp.group(0)))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"98dc0552-45b6-4f5f-a7ab-c98d6ac736d3","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"75.6\n"}]},{"source":"## Vectorizing text\nTF/IDF: Vectorizes words based upon importance\n- TF = Term Frequency\n- IDF = Inverse Document Frequency","metadata":{},"cell_type":"markdown","id":"0ea85a2c-a139-4cbd-8904-d7da93ded0c9"},{"source":"## Vectorizing text","metadata":{},"cell_type":"markdown","id":"757984d9-4a50-4e60-82a1-20ee810d5c78"},{"source":"from sklearn.feature_extraction.text import TfidfVectorizer\nprint(documents.head())\ntfidf_vec = TfidfVectorizer()\ntext_tfidf = tfidf_vec.fit_transform(documents)","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1693412022467,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"a899901b-c54e-46e5-a9f5-ece94851a7ea","execution_count":24,"outputs":[]},{"source":"## Text classification\n\n$$\nP(A∣B)= \\frac{P(B∣A) P(A)}{P(B)}\n$$","metadata":{},"cell_type":"markdown","id":"5eeb8f1a-3354-4517-8dcf-543b748d7dd2"},{"source":"## Extracting string patterns\nThe Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame.\n\n### Instructions\n\n- Search the text in the length argument for numbers and decimals using an appropriate pattern.\n- Extract the matched pattern and convert it to a float.\n- Apply the `return_mileage()` function to each row in the `hiking[\"Length\"]` column.","metadata":{},"cell_type":"markdown","id":"c626ae18-cff4-4745-9608-84ce94382ba3"},{"source":"hiking_new = pd.read_csv('datasets/hiking.csv', index_col=[0])\n\n# Write a pattern to extract numbers and decimals\ndef return_mileage(length):\n    \n    # Search the text for matches\n    mile = re.search(\"\\d+\\.\\d+\", length)\n    \n    # If a value is returned, use group(0) to return the found value\n    if mile is not None:\n        return float(mile.group(0))\n        \n# Apply the function to the Length column and take a look at both columns\nhiking_new[\"Length_num\"] = hiking_new[\"Length\"].apply(return_mileage)\nprint(hiking_new[[\"Length\", \"Length_num\"]].head())","metadata":{"executionCancelledAt":null,"executionTime":25,"lastExecutedAt":1693412030838,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"hiking_new = pd.read_csv('datasets/hiking.csv', index_col=[0])\n\n# Write a pattern to extract numbers and decimals\ndef return_mileage(length):\n    \n    # Search the text for matches\n    mile = re.search(\"\\d+\\.\\d+\", length)\n    \n    # If a value is returned, use group(0) to return the found value\n    if mile is not None:\n        return float(mile.group(0))\n        \n# Apply the function to the Length column and take a look at both columns\nhiking_new[\"Length_num\"] = hiking_new[\"Length\"].apply(return_mileage)\nprint(hiking_new[[\"Length\", \"Length_num\"]].head())","outputsMetadata":{"0":{"height":138,"type":"stream"}}},"cell_type":"code","id":"1bb1f485-ba8c-4f85-9591-43cef5bfad04","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":"       Length  Length_num\n0   0.8 miles        0.80\n1    1.0 mile        1.00\n2  0.75 miles        0.75\n3   0.5 miles        0.50\n4   0.5 miles        0.50\n"}]},{"source":"## Vectorizing text\n\nYou'll now transform the volunteer dataset's title column into a text vector, which you'll use in a prediction task in the next exercise.\n\n### Instructions\n\n- Store the `volunteer[\"title\"]` column in a variable named `title_text`.\n- Instantiate a `TfidfVectorizer` as `tfidf_vec`.\n- Transform the text in `title_text` into a `tf-idf` vector using `tfidf_vec`","metadata":{},"cell_type":"markdown","id":"ae4a2daa-cdb6-4c44-914e-6cc98082cddf"},{"source":"# Take the title text\ntitle_text = volunteer[\"title\"]\n\n# Create the vectorizer method\ntfidf_vec = TfidfVectorizer()\n\n# Transform the text into tf-idf vectors\ntext_tfidf = tfidf_vec.fit_transform(title_text)","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1693412037293,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Take the title text\ntitle_text = volunteer[\"title\"]\n\n# Create the vectorizer method\ntfidf_vec = TfidfVectorizer()\n\n# Transform the text into tf-idf vectors\ntext_tfidf = tfidf_vec.fit_transform(title_text)"},"cell_type":"code","id":"99fdf1b9-e9b6-4165-83b7-c96b20820a2c","execution_count":26,"outputs":[]},{"source":"## Text classification using tf/idf vectors\n\nNow that you've encoded the volunteer dataset's title column into tf/idf vectors, you'll use those vectors to predict the `category_desc` column.\n\n### Instructions\n\n- Split the `text_tfidf` vector and `y` target variable into training and test sets, setting the stratify parameter equal to `y`, since the class distribution is uneven. Notice that we have to run the `.toarray()` method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n- Fit the `X_train` and `y_train` data to the Naive Bayes model, `nb`.\n- Print out the test set accuracy.","metadata":{},"cell_type":"markdown","id":"d500c233-ed30-47a4-90f8-e9e7de743a40"},{"source":"# Import MultinomialNB\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n# Split the dataset according to the class distribution of category_desc\ny = volunteer[\"category_desc\"]\nX_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n\n# Fit the model to the training data\nnb.fit(X_train, y_train)\n\n# Print out the model's accuracy\nprint(nb.score(X_test, y_test))","metadata":{"executionCancelledAt":null,"executionTime":82,"lastExecutedAt":1693412042715,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import MultinomialNB\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n# Split the dataset according to the class distribution of category_desc\ny = volunteer[\"category_desc\"]\nX_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n\n# Fit the model to the training data\nnb.fit(X_train, y_train)\n\n# Print out the model's accuracy\nprint(nb.score(X_test, y_test))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"506cc1f5-2411-42a9-979d-73bf3500ef05","execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":"0.5548387096774193\n"}]},{"source":"# PART FOUR","metadata":{},"cell_type":"markdown","id":"677a53a7-d824-4c93-8bc0-f1afbc0dd4c5"},{"source":"## 1 Feature selection\n\n## What is feature selection?\n\n- Selecting features to be used for modeling\n- Doesn't create new features\n- Improve model's performance\n\n## When to select features\n\n|city  |state  |lat  |long|\n|---  |---  |---  |---|\n|hico  |tx  |31.982778  |-98.033333|\n|mackinaw city  |mi  |45.783889  |-84.727778|\n|winchester  |ky  |37.990000  |-84.179722|\n\n- Reducing noice\n- Features are strongly statistically correlated\n- Reduce overall variance","metadata":{},"cell_type":"markdown","id":"cfd0cca7-8c7c-410b-a1ae-ef5bab82eda6"},{"source":"## Selecting relevant features\nIn this exercise, you'll identify the redundant columns in the volunteer dataset, and perform feature selection on the dataset to return a DataFrame of the relevant features.\nFor example, if you explore the volunteer dataset in the console, you'll see three features which are related to location: locality, region, and postalcode. They contain related information, so it would make sense to keep only one of the features.\n\nTake some time to examine the features of volunteer in the console, and try to identify the redundant features.\n\n### Instructions\n\n- Create a list of redundant column names and store it in the to_drop variable:\n- Out of all the location-related features, keep only postalcode.\n- Features that have gone through the feature engineering process are redundant as well.\n- Drop the columns in the to_drop list from the dataset.\n- Print out the `.head()` of `volunteer_subset` to see the selected columns.","metadata":{},"cell_type":"markdown","id":"8a2832ff-641a-4901-bf8d-05d7893b1659"},{"source":"# Create a list of redundant column names to drop\nto_drop = [\"vol_requests\", \"category_desc\", \"locality\", \"region\", \"created_date\"]\n\n# Drop those columns from the dataset\nvolunteer_subset = volunteer.drop(to_drop, axis=1)\n\n# Print out the head of volunteer_subset\nprint(volunteer_subset.head())","metadata":{"executionCancelledAt":null,"executionTime":42,"lastExecutedAt":1693412053157,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a list of redundant column names to drop\nto_drop = [\"vol_requests\", \"category_desc\", \"locality\", \"region\", \"created_date\"]\n\n# Drop those columns from the dataset\nvolunteer_subset = volunteer.drop(to_drop, axis=1)\n\n# Print out the head of volunteer_subset\nprint(volunteer_subset.head())","outputsMetadata":{"0":{"height":178,"type":"stream"}}},"cell_type":"code","id":"96840108-24bd-4d7f-9148-299697eccc8b","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"   Unnamed: 0  opportunity_id  ...  start_date_converted  start_date_month\n0           1            5008  ...            2011-02-01                 2\n1           2            5016  ...            2011-01-29                 1\n2           3            5022  ...            2011-02-14                 2\n3           4            5055  ...            2011-02-05                 2\n4           5            5056  ...            2011-02-12                 2\n\n[5 rows x 33 columns]\n"}]},{"source":"## 2 Removing redundant features\n\n## Redundant features\n- Remove noisy features\n- Remove correlated features\n- Remove duplicated features\n\n## Scenarios for manual removal\n\n|city  |state  |lat  |long|\n|---  |---  |---  |---|\n|hico  |tx  |31.982778  |-98.033333|\n|mackinaw city  |mi  |45.783889  |-84.727778|\n|winchester  |ky  |37.990000  |-84.179722|\n\n## Correlated features\n- Statistically correlated: features move together directionally\n- Linear models assume feature independence\n- Pearson's correlation coefficient","metadata":{},"cell_type":"markdown","id":"2f62e144-4fc3-4847-b574-3ef15b07e56e"},{"source":"## Correlated features","metadata":{},"cell_type":"markdown","id":"9497475a-606e-4a07-8353-8dffd3794f79"},{"source":"print(df)\nprint(df.corr())","metadata":{"executionCancelledAt":1693411809045},"cell_type":"code","id":"a8d4609d-6379-4df8-851f-4f4597f50c79","execution_count":null,"outputs":[]},{"source":"## Checking for correlated features\nYou'll now return to the wine dataset, which consists of continuous, numerical features. Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.\n\n### Instructions\n\n- Print out the Pearson correlation coefficients for each pair of features in the wine dataset.\n- Drop any columns from wine that have a correlation coefficient above 0.75 with at least two other columns.","metadata":{},"cell_type":"markdown","id":"4c5f98f1-b962-4244-883a-048f8c7dad8e"},{"source":"# Print out the column correlations of the wine dataset\nprint(wine.corr())\n\n# Drop that column from the DataFrame\nwine = wine.drop(['Flavanoids'], axis = 1)\n\nprint(wine.head())","metadata":{"executionCancelledAt":null,"executionTime":39,"lastExecutedAt":1693413524110,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print out the column correlations of the wine dataset\nprint(wine.corr())\n\n# Drop that column from the DataFrame\nwine = wine.drop(['Flavanoids'], axis = 1)\n\nprint(wine.head())","outputsMetadata":{"0":{"height":520,"type":"stream"}}},"cell_type":"code","id":"66604a66-512b-4336-a1a9-998c6361b02e","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":"                                  Type  ...   Proline\nType                          1.000000  ... -0.633717\nAlcohol                      -0.328222  ...  0.643720\nMalic acid                    0.437776  ... -0.192011\nAsh                          -0.049643  ...  0.223626\nAlcalinity of ash             0.517859  ... -0.440597\nMagnesium                    -0.209179  ...  0.393351\nTotal phenols                -0.719163  ...  0.498115\nFlavanoids                   -0.847498  ...  0.494193\nNonflavanoid phenols          0.489109  ... -0.311385\nProanthocyanins              -0.499130  ...  0.330417\nColor intensity               0.265668  ...  0.316100\nHue                          -0.617369  ...  0.236183\nOD280/OD315 of diluted wines -0.788230  ...  0.312761\nProline                      -0.633717  ...  1.000000\n\n[14 rows x 14 columns]\n   Type  Alcohol  Malic acid  ...   Hue  OD280/OD315 of diluted wines  Proline\n0     1    14.23        1.71  ...  1.04                          3.92     1065\n1     1    13.20        1.78  ...  1.05                          3.40     1050\n2     1    13.16        2.36  ...  1.03                          3.17     1185\n3     1    14.37        1.95  ...  0.86                          3.45     1480\n4     1    13.24        2.59  ...  1.04                          2.93      735\n\n[5 rows x 13 columns]\n"}]},{"source":"## 3 Selecting features using text vectors\n\n## Looking at word weights","metadata":{},"cell_type":"markdown","id":"d77d725d-33a4-425b-8c0f-b2ff17713694"},{"source":"print(tfidf_vec.vocabulary_)\nprint(text_tfidf[3].data)\nprint(text_tfidf[3].indices)","metadata":{"executionCancelledAt":1693411809045},"cell_type":"code","id":"dc90eda2-b2da-4ba6-a426-2377f5e305e7","execution_count":null,"outputs":[]},{"source":"## Looking at word weights","metadata":{},"cell_type":"markdown","id":"c9250177-2ece-4951-bfdf-1fbe8ffbf620"},{"source":"vocab = {v:k for k,v intfidf_vec.vocabulary_.items()}\nprint(vocab)\nzipped_row = dict(zip(text_tfidf[3].indices,text_tfidf[3].data))\nprint(zipped_row)\n\ndef return_weights(vocab, vector, vector_index):    \n    zipped = dict(zip(vector[vector_index].indices,                       \n                      vector[vector_index].data))\n    return {vocab[i]:zipped[i] for i in vector[vector_index].indices}\nprint(return_weights(vocab, text_tfidf, 3))","metadata":{"executionCancelledAt":1693411809046},"cell_type":"code","id":"bc4b90da-444e-4b6a-9477-5dff41193f4a","execution_count":null,"outputs":[]},{"source":"## Exploring text vectors, part 1\nLet's expand on the text vector exploration method we just learned about, using the volunteer dataset's title tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector.\n\n### Instructions\n\n- Add parameters called original_vocab, for the tfidf_vec.vocabulary_, and top_n.\n- Call pd.Series() on the zipped dictionary. This will make it easier to operate on.\n- Use the .sort_values() function to sort the series and slice the index up to top_n words.\n- Call the function, setting original_vocab=tfidf_vec.vocabulary_, setting vector_index=8 to grab the 9th row, and setting top_n=3, to grab the top 3 weighted words.","metadata":{},"cell_type":"markdown","id":"076350c6-7ccd-4015-b177-a3d67cd8f673"},{"source":"# Add in the rest of the arguments\ndef return_weights(vocab, original_vocab, vector, vector_index, top_n):\n    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n    \n    \n    # Transform that zipped dict into a series\n    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n    \n    # Sort the series to pull out the top n weighted words\n    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n    return [original_vocab[i] for i in zipped_index]\n\n# Print out the weighted words\nprint(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1693414214433,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"0365addd-738a-457b-a432-2a0a101834f1","execution_count":33,"outputs":[]},{"source":"## Exploring text vectors, part 2\nUsing the return_weights() function you wrote in the previous exercise, you're now going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.\n\n### Instructions\n\n- Call `return_weights()` to return the top weighted words for that document.\n- Call `set()` on the returned filter_list to remove duplicated numbers.\n- Call `words_to_filter`, passing in the following parameters: vocab for the vocab parameter, `tfidf_vec.vocabulary_` for the `original_vocab` parameter, `text_tfidf` for the vector parameter, and 3 to grab the top_n 3 weighted words from each document.\n- Finally, pass that `filtered_words` set into a list to use as a filter for the text vector.","metadata":{},"cell_type":"markdown","id":"a4cfa358-83b0-4439-946d-888ab6b8745c"},{"source":"def words_to_filter(vocab, original_vocab, vector, top_n):\n    filter_list = []\n    for i in range(0, vector.shape[0]):\n    \n        # Call the return_weights function and extend filter_list\n        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n        filter_list.extend(filtered)\n        \n    # Return the list in a set, so we don't get duplicate word indices\n    return set(filter_list)\n\n# Call the function to get the list of word indices\nfiltered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n\n# Filter the columns in text_tfidf to only those in filtered_words\nfiltered_text = text_tfidf[:, list(filtered_words)]","metadata":{},"cell_type":"code","id":"6b083541-0a44-4b45-bd65-3375f3ff0fb9","execution_count":null,"outputs":[]},{"source":"## Training Naive Bayes with feature selection\nYou'll now re-run the Naive Bayes text classification model that you ran at the end of Chapter 3 with our selection choices from the previous exercise: the volunteer dataset's title and category_desc columns.\n\n### Instructions\n\n- Use `train_test_split()` on the `filtered_text` text vector, the `y` labels (which is the `category_desc` labels), and pass the `y` set to the stratify parameter, since we have an uneven class distribution.\n- Fit the `nb` Naive Bayes model to `X_train` and `y_train`.\n- Calculate the test set accuracy of `nb`.","metadata":{},"cell_type":"markdown","id":"3103d4da-02cc-4c7d-835a-3479865fd200"},{"source":"# Split the dataset according to the class distribution of category_desc\nX_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n\n# Fit the model to the training data\nnb.fit(X_train, y_train)\n\n# Print out the model's accuracy\nprint(nb.score(X_test, y_test))","metadata":{},"cell_type":"code","id":"2945d784-81f9-4ea8-9320-88aecb15c16e","execution_count":null,"outputs":[]},{"source":"## 4 Dimensionality reduction\n## Dimensionality reduction and PCA\n         \n- Unsupervised learning method              - Principal component analysis        \n- Combines/decomposes a feature space        - Linear transformation to uncorrelated space \n- Feature extraction - here we'll use to reduce our feature space       - Captures as much variance as possible ineach component\n\n","metadata":{},"cell_type":"markdown","id":"379ed036-0cdb-478f-be19-c1b12b806a1e"},{"source":"## PCA in scikit-learn","metadata":{},"cell_type":"markdown","id":"9d5ec4cb-e328-4c56-93c3-2450b254dc5f"},{"source":"from sklearn.decomposition import PCA\npca = PCA()\ndf_pca = pca.fit_transform(df)\nprint(df_pca)\nprint(pca.explained_variance_ratio_)","metadata":{"executionCancelledAt":1693411809046},"cell_type":"code","id":"ac5c8536-7c32-4c40-8901-d9b0762f0dd4","execution_count":null,"outputs":[]},{"source":"## PCA caveats\n- Difficult to interpret components\n- End of preprocessing journey\n","metadata":{},"cell_type":"markdown","id":"60532758-7486-49ce-bc1b-fcf476756959"},{"source":"## Using PCA\nIn this exercise, you'll apply PCA to the wine dataset, to see if you can increase the model's accuracy.\n\n### Instructions\n\n- Instantiate a PCA object.\n- Define the features (X) and labels (y) from wine, using the labels in the \"Type\" column.\n- Apply PCA to X_train and X_test, ensuring no data leakage, and store the transformed values as pca_X_train and pca_X_test.\n- Print out the .explained_variance_ratio_ attribute of pca to check how much variance is explained by each component.","metadata":{},"cell_type":"markdown","id":"74973792-be37-46c8-ad92-e0fdd536fa47"},{"source":"from sklearn.decomposition import PCA\npca = PCA()\n# Instantiate a PCA object\npca = PCA()\n\n# Define the features and labels from the wine dataset\nX = wine.drop(['Type'], axis=1)\ny = wine[\"Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Apply PCA to the wine dataset X vector\npca_X_train = pca.fit_transform(X_train)\npca_X_test = pca.transform(X_test)\n\n# Look at the percentage of variance explained by the different components\nprint(pca.explained_variance_ratio_)","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1693415167958,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.decomposition import PCA\npca = PCA()\n# Instantiate a PCA object\npca = PCA()\n\n# Define the features and labels from the wine dataset\nX = wine.drop(['Type'], axis=1)\ny = wine[\"Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# Apply PCA to the wine dataset X vector\npca_X_train = pca.fit_transform(X_train)\npca_X_test = pca.transform(X_test)\n\n# Look at the percentage of variance explained by the different components\nprint(pca.explained_variance_ratio_)","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"cell_type":"code","id":"6cf4feae-a98f-4dc2-81e4-f538b82b0055","execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":"[9.97802349e-01 2.02071713e-03 9.82348559e-05 5.53994004e-05\n 1.10395648e-05 5.87233448e-06 3.13858204e-06 1.54420449e-06\n 1.02927386e-06 3.90521513e-07 1.95535151e-07 8.99659634e-08]\n"}]},{"source":"## Training a model with PCA\nNow that you have run PCA on the wine dataset, you'll finally train a KNN model using the transformed data.\n\n### Instructions\n\n- Fit the `knn` model to the PCA-transformed features, `pca_X_train`, and training labels, `y_train`.\n- Print the test set accuracy of the `knn` model using `pca_X_test` and `y_test`.","metadata":{},"cell_type":"markdown","id":"6db89787-6f1e-4629-9950-63fef9ceebb9"},{"source":"knn = KNeighborsClassifier()\n# Fit knn to the training data\nknn.fit(pca_X_train, y_train)\n\n# Score knn on the test data and print it out\nprint(knn.score(pca_X_test, y_test))\n","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1693415307411,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"knn = KNeighborsClassifier()\n# Fit knn to the training data\nknn.fit(pca_X_train, y_train)\n\n# Score knn on the test data and print it out\nprint(knn.score(pca_X_test, y_test))\n","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"21af79dc-bf33-492f-b131-e085611967d9","execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":"0.7777777777777778\n"}]},{"source":"# PART FIVE","metadata":{},"cell_type":"markdown","id":"c98d328b-b4fc-4208-b91f-34c93802e399"},{"source":"## 1 UFOs and preprocessing\n\n## Identifying areas for preprocessing\n\n\n### Important concepts to remember\n- Missing data: `.dropna()` and `.isna()`\n- Types: `.astype()`\n- Stratified sampling: `train_test_split(X, y, stratify=y)`\n","metadata":{},"cell_type":"markdown","id":"bb282233-aa88-44f4-bbc6-30c0547274ce"},{"source":"","metadata":{},"cell_type":"markdown","id":"c120f59c-217a-467c-9163-831a030c1dde"},{"source":"## 2 Categorical variables and standardization\n\n## Categorical variables\n\n|   |state |country       |type|\n|-----|--------|--------|------|\n|295    |az      |us     | light|\n|296    |tx      |us  |formation|\n|297    |nv      |us   |fireball|\n\n- One-hot encoding: `pd.get_dummies()`\n\n## Standardization\n- `.var()`\n- `np.log()`","metadata":{},"cell_type":"markdown","id":"1efe748a-1dc3-4d35-b398-64e75477130e"},{"source":"","metadata":{},"cell_type":"markdown","id":"f0a4ccfd-d78d-4cbe-9a93-bf9cf3728795"},{"source":"## 3 Engineering new features\n\n## UFO feature engineering\n\n|date  |length_of_time  |desc|\n|----|-----|-----|\n|6/16/2013 |21005 minutes  |Sabino Canyon Tucson Arizona night UFO sighting.|\n|9/12/2005 |22355 minutes  |Star like objects hovering in sky, slowly m...|\n|12/31/2013 |22253 minutes  |Three orange fireballs spotted by witness in E...|\n\n- Dates: `.dt.month` or `.dt.hour` attributes\n- Regex: `\\d` and `.group()`\n- Text: tf-idf and `TfidfVectorizer`","metadata":{},"cell_type":"markdown","id":"e047a85d-b47e-4b69-a27c-67eb82c4762e"},{"source":"- ","metadata":{},"cell_type":"markdown","id":"b868d42f-6114-4680-8a0c-7e192b143bc4"},{"source":"## 4 Feature selection and modeling\n\n## Feature selection and modeling\n- Redundant features\n- Text vector\n\n## Final thoughts\n- Iterative processes\n- Know your dataset\n- Understand your modeling task\n","metadata":{},"cell_type":"markdown","id":"f1d9c3d2-db54-4fec-98d7-f2ab935ce0c7"},{"source":"## What you've learned\n- Preparing data for modeling:\n    - Missing dataIncorrect types\n    - Standardize numerical values\n    - Process categorical values\n    - Feature engineering\n    - Select features for modeling","metadata":{},"cell_type":"markdown","id":"2b5ebd7f-06ad-4c0a-8838-357ef92e8042"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}